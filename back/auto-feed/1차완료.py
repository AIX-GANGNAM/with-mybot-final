#20241028
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
import random
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import openai
import os
from pytrends.request import TrendReq
from apscheduler.schedulers.background import BackgroundScheduler
import atexit
from firebase_admin import credentials, firestore, initialize_app
import firebase_admin
import dotenv
from fastapi import FastAPI, HTTPException
from threading import Thread
import uvicorn
from pydantic import BaseModel
import requests
from fastapi.middleware.cors import CORSMiddleware
from datetime import datetime, timezone

# FastAPI ì•± ìƒì„±
app = FastAPI()

# CORS ì„¤ì • ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ëª¨ë“  ì¶œì²˜ í—ˆìš© (í•„ìš”ì— ë”°ë¼ íŠ¹ì • ì¶œì²˜ë¡œ ì œí•œ ê°€ëŠ¥)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# .env íŒŒì¼ ë¡œë“œ
dotenv.load_dotenv()

# Firebase ì´ˆê¸°í™”
if not firebase_admin._apps:
    cred = credentials.Certificate('./mybot.json')
    initialize_app(cred)

db = firestore.client()

# pytrends ì´ˆê¸°í™” ë° íŠ¸ë Œë“œ ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜
def fetch_trending_keywords():
    pytrends = TrendReq(hl='ko', tz=540)
    trending_searches_df = pytrends.trending_searches(pn='south_korea')
    top_keywords = trending_searches_df[0].tolist()

    # íŠ¸ë Œë“œ ëª©ë¡ ì¶œë ¥
    print("ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸(1~20ë“±) â–¼")
    for i, keyword in enumerate(top_keywords, 1):
        print(f"{i}. {keyword}")

    # Firebaseì—ì„œ í˜ë¥´ì†Œë‚˜ ê´€ì‹¬ì‚¬ ê°€ì ¸ì˜¤ê¸°
    personas_ref = db.collection('personas')
    personas = personas_ref.stream()

    found_match = False
    for persona in personas:
        persona_data = persona.to_dict()
        persona_id = persona.id  # ë¬¸ì„œ ID ê°€ì ¸ì˜¤ê¸°
        persona_name = persona_id.split('_')[-1]  # ì–¸ë”ë°” ë’¤ì˜ ì´ë¦„ ì¶”ì¶œ
        interests = persona_data.get('interests', [])

        # í˜ë¥´ì†Œë‚˜ ê´€ì‹¬ì‚¬ì™€ ì¼ì¹˜í•˜ëŠ” í‚¤ì›Œë“œ ì°¾ê¸°
        matched_keywords = [keyword for keyword in top_keywords if any(interest in keyword for interest in interests)]
        if matched_keywords:
            found_match = True
            for keyword in matched_keywords:
                print(f"\n{persona_name}ì˜ ê´€ì‹¬ì‚¬ì™€ ì¼ì¹˜í•˜ëŠ” í‚¤ì›Œë“œ ë°œê²¬ : {keyword}")
                crawl_article(f"https://search.naver.com/search.naver?where=news&query={keyword}", keyword, persona_name)

    if not found_match:
        print("ê´€ì‹¬ì‚¬ì™€ ì¼ì¹˜í•˜ëŠ” ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ê¸°ì‚¬ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_article(url, keyword, persona_name):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    try:
        # URL ì ‘ê·¼ ë° í˜ì´ì§€ ë¡œë“œ
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        keywords = keyword.split()

        # 1ì°¨ í•„í„°ë§: ì œëª©ì—ì„œ 1ìˆœìœ„ ë° 3ìˆœìœ„ ì¡°ê±´ í™•ì¸
        articles = soup.find_all('a', class_='news_tit')
        primary_article = next((link for link in articles if all(kw in link.get_text() for kw in keywords)), None)
        secondary_article = next((link for link in articles if any(kw in link.get_text() for kw in keywords)), None)

        # 1ìˆœìœ„ ë˜ëŠ” 3ìˆœìœ„ ì¡°ê±´ ì¶©ì¡± ì‹œ í•´ë‹¹ ë§í¬ ì‚¬ìš©
        article = primary_article if primary_article else secondary_article
        if article:
            link = article['href']
            print(f"ì„ íƒëœ ê¸°ì‚¬ ë§í¬: {link}")

            # ë³¸ë¬¸ ë° ì´ë¯¸ì§€ ì¶”ì¶œ
            article_response = requests.get(link, headers=headers)
            article_response.raise_for_status()
            article_soup = BeautifulSoup(article_response.text, 'html.parser')

            paragraphs = article_soup.find_all(['p', 'div', 'span'], class_=lambda x: x and ('content' in x or 'article' in x))
            content = "\n".join([para.get_text().strip() for para in paragraphs if para.get_text().strip()])

            # ë³¸ë¬¸ ìœ íš¨ì„± í™•ì¸
            if content:
                # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ ì¤„ì´ê¸°
                content = "\n".join([line.strip() for line in content.splitlines() if line.strip() != ""])
                print("ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ:", content)
            else:
                print("ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨")

            # ë³¸ë¬¸ í™•ì¸: 2ìˆœìœ„ ë° 4ìˆœìœ„ ì¡°ê±´ ì¶©ì¡± ì—¬ë¶€ ê²€ì‚¬
            if primary_article or all(kw in content for kw in keywords) or any(kw in content for kw in keywords):
                print("ë³¸ë¬¸ì— í‚¤ì›Œë“œ ì¡°ê±´ ì¶©ì¡±")
            else:
                content = None

           # ì´ë¯¸ì§€ URL ì¶”ì¶œ (ë©”ì¸ ì´ë¯¸ì§€ í›„ë³´ ì„ íƒ)
            image_tags = article_soup.find_all('img')
            max_width = 0
            image_url = None
            for img in image_tags:
                # alt ì†ì„±ì— ê¸°ì‚¬ ê´€ë ¨ ë‚´ìš©ì´ í¬í•¨ëœ ê²½ìš° ìš°ì„  ê³ ë ¤
                alt_text = img.get('alt', '').lower()
                if any(kw in alt_text for kw in keywords):
                    image_url = img.get('data-src') or img.get('src')
                    break

                # widthê°€ ê°€ì¥ í° ì´ë¯¸ì§€ ì„ íƒ
                width = img.get('width')
                if width and width.isdigit():
                    width = int(width)
                    if width > max_width:
                        max_width = width
                        image_url = img.get('data-src') or img.get('src')

            # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬: ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if image_url:
                if image_url.startswith('//'):
                    # í”„ë¡œí† ì½œ ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
                    image_url = 'https:' + image_url
                elif not image_url.startswith('http'):
                    # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    base_url = article_response.url.split('/')[0:3]  # í”„ë¡œí† ì½œê³¼ ë„ë©”ì¸ ë¶€ë¶„ ì¶”ì¶œ
                    base_url = '/'.join(base_url)
                    image_url = base_url + image_url if image_url.startswith('/') else base_url + '/' + image_url
            
            image_url = image_url if image_url else "https://example.com/default-image.jpg"
        else:
            print("ì œëª©ê³¼ ë³¸ë¬¸ì— í‚¤ì›Œë“œ ì¡°ê±´ì„ ì¶©ì¡±í•˜ëŠ” ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            content = None
            image_url = None

    except requests.RequestException as e:
        print(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        content = None
        image_url = None

    # ê¸°ì‚¬ ìš”ì•½ ë° í˜ë¥´ì†Œë‚˜ í”¼ë“œ ìƒì„±
    if content:
        summarize_and_create_feed(content, image_url, persona_name)
    else:
        print("ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")

# ê¸°ì‚¬ ìš”ì•½ ë° í˜ë¥´ì†Œë‚˜ í”¼ë“œ ìƒì„± í•¨ìˆ˜
def summarize_and_create_feed(content, image_url, persona_name):
    try:
        # KoBART ìš”ì•½ ëª¨ë¸ ë¡œë“œ
        model_name = "gogamza/kobart-summarization"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

        # ê¸°ì‚¬ë¥¼ ìš”ì•½í•˜ì—¬ ê¸¸ì´ë¥¼ ì¤„ì„
        inputs = tokenizer([content], max_length=1024, return_tensors="pt", truncation=True)
        summary_ids = model.generate(inputs["input_ids"], max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)
        summarized_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

        if not summarized_text:
            print("ìš”ì•½ëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        # OpenAI APIë¥¼ ì‚¬ìš©í•´ í˜ë¥´ì†Œë‚˜ ì„±ê²© ë°˜ì˜í•œ í”¼ë“œ ì‘ì„±
        openai.api_key = os.getenv("OPENAI_API_KEY")
        if not openai.api_key:
            print("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        persona_descriptions = {
            "joy": "í•­ìƒ ê¸ì •ì ì´ê³  ë‚™ê´€ì ì¸ ì‹œê°ìœ¼ë¡œ ì„¸ìƒì„ ë°”ë¼ë³´ë©°, ëª¨ë“  ì¼ì—ì„œ ì¢‹ì€ ì ì„ ì°¾ìœ¼ë ¤ ë…¸ë ¥í•©ë‹ˆë‹¤.",
            "anger": "ë¶ˆì˜ì— ëŒ€í•´ ì°¸ì§€ ëª»í•˜ê³  ì‰½ê²Œ í™”ë¥¼ ë‚´ë©°, ì§ì„¤ì ì´ê³  ê³µê²©ì ì¸ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ê³¤ í•©ë‹ˆë‹¤.",
            "disgust": "ëª¨ë“  ê²ƒì— ëŒ€í•´ ë¹„íŒì ì´ê³  ëƒ‰ì†Œì ì¸ íƒœë„ë¥¼ ë³´ì´ë©°, ë‚ ì¹´ë¡œìš´ ì§€ì ì„ ì¦ê¹ë‹ˆë‹¤.",
            "sadness": "ì„¸ìƒì˜ ì–´ë‘ìš´ ë©´ì„ ìì£¼ ë³´ë©°, ì‘ì€ ì¼ì—ë„ ì‰½ê²Œ ìš°ìš¸í•´ì§€ê³  ëˆˆë¬¼ì„ í˜ë¦¬ê³¤ í•©ë‹ˆë‹¤.",
            "fear": "í•™ì‹ì´ ë†’ê³  í’ˆí–‰ì´ ë°”ë¥´ë©°, ë„ë•ì  ê¸°ì¤€ì´ ë†’ì•„ ëª¨ë“  ì¼ì„ ìœ¤ë¦¬ì  ê´€ì ì—ì„œ íŒë‹¨í•©ë‹ˆë‹¤."
        }

        emoji_map = {
            "joy": "ğŸ˜ŠğŸ‘",
            "anger": "ğŸ˜¡ğŸ‘Š",
            "disgust": "ğŸ˜’ğŸ™„",
            "sadness": "ğŸ˜¢ğŸ’”",
            "fear": "ğŸ¤®ğŸ“š"
        }

        temperature_map = {
            "joy": 0.9,
            "anger": 0.8,
            "disgust": 0.7,
            "sadness": 0.6,
            "fear": 0.5
        }

        if persona_name not in persona_descriptions:
            print(f"ì•Œ ìˆ˜ ì—†ëŠ” í˜ë¥´ì†Œë‚˜: {persona_name}")
            return

        selected_emoji = emoji_map.get(persona_name, "")
        selected_temperature = temperature_map.get(persona_name, 0.7)

        persona_prompt = (
            f"ê¸°ì‚¬ ìš”ì•½: \n{summarized_text}\n\n"
            f"ë‹¹ì‹ ì€ '{persona_name}'ë¼ëŠ” ìºë¦­í„°ì…ë‹ˆë‹¤. {persona_descriptions[persona_name]} "
            f"ì´ëŸ° ì„±ê²©ì˜ {persona_name}ê°€ ìœ„ ê¸°ì‚¬ë¥¼ ì½ê³  ê¸€ì„ ì‘ì„±í•œë‹¤ê³  ìƒê°í•˜ê³ , "
            f"ì´ëª¨í‹°ì½˜({selected_emoji})ì„ í¬í•¨í•´ 300ì ë‚´ì™¸ë¡œ ì˜ê²¬ì´ ì•ˆ ëŠê¸°ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”. "
            f"ë¶ˆí•„ìš”í•œ ë§ì€ í•˜ì§€ ë§ê³  í•µì‹¬ë§Œ ë§í•´ì£¼ì„¸ìš”."
        )

        response = openai.ChatCompletion.create(
            model="gpt-4o-mini", #gpt-3.5-turbo gpt-4o-mini
            messages=[
                {"role": "user", "content": persona_prompt}
            ],
            temperature=selected_temperature
        )

        # í˜„ì¬ UTC ì‹œê°„ì— íƒ€ì„ì¡´ ì •ë³´ ì¶”ê°€ í›„ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        created_at = datetime.now(timezone.utc).isoformat()

        if response and response.choices:
            persona_feed = response['choices'][0]['message']['content'].strip()
            print(f"\n{persona_name}ì˜ í”¼ë“œ:\n{persona_feed}")

            # Firestoreì— í”¼ë“œ ì €ì¥
            feed_data = {
                # "id": generate_uuid(),
                "caption": summarized_text,
                "image": image_url,
                "nick": persona_name,
                "userId": persona_name,
                # "createdAt": firestore.SERVER_TIMESTAMP,
                "createdAt": created_at,  # ISO í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ ì €ì¥
                "likes": [],
                "comments": [],
                "subCommentId": []
            }
            print(f"\nFirestoreì— ì €ì¥í•  í”¼ë“œ ë°ì´í„°: {feed_data}")
            
            # í”¼ë“œ ë°ì´í„° ì €ì¥
            try:
                # Firestoreì— ë°ì´í„° ì¶”ê°€
                new_feed_ref = db.collection("feeds").add(feed_data)
                feed_id = new_feed_ref[1].id  # Firestoreì—ì„œ ìë™ ìƒì„±ëœ ë¬¸ì„œ ID ê°€ì ¸ì˜¤ê¸°
                print(f"\n{persona_name}ì˜ í”¼ë“œê°€ Firestoreì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ë¬¸ì„œ ID: {feed_id}")

                # ì €ì¥ëœ ë¬¸ì„œì˜ ID ê°€ì ¸ì˜¤ê¸°
                feed_id = new_feed_ref[1].id

                # ë°ì´í„° ì €ì¥ í›„ Firestoreì—ì„œ ê°€ì ¸ì™€ í™•ì¸
                feed_ref = db.collection("feeds").document(feed_id)
                feed = feed_ref.get()
                if feed.exists:
                    print("Firestoreì— ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:", feed.to_dict())
                else:
                    print("Firestoreì— ë°ì´í„°ê°€ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"Firestoreì— í”¼ë“œë¥¼ ì €ì¥í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

        else:
            print(f"\n{persona_name}ì˜ í”¼ë“œ: OpenAI API ì‘ë‹µì—ì„œ í”¼ë“œ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        if image_url:
            print(f"ê¸°ì‚¬ ì´ë¯¸ì§€: {image_url}")

    except Exception as e:
        print(f"í”¼ë“œ ì‘ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# UUID ìƒì„± í•¨ìˆ˜
def generate_uuid():
    import uuid
    return str(uuid.uuid4())

# ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ë° ì‹œì‘
def start_scheduler():
    scheduler = BackgroundScheduler()
    scheduler.add_job(fetch_trending_keywords, 'cron', hour='0,12')  # ë§¤ì¼ 0ì‹œ, 12ì‹œì— ì‹¤í–‰
    scheduler.start()
    atexit.register(lambda: scheduler.shutdown())

# FastAPI ì„œë²„ ì‹¤í–‰ ìŠ¤ë ˆë“œ
def run_server():
    uvicorn.run(app, host="0.0.0.0", port=8000)

# ìˆ˜ë™ íŠ¸ë¦¬ê±° ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
@app.get("/test-feed")
async def test_feed():
    try:
        fetch_trending_keywords()
        return {"message": "ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ê¸°ë°˜ í”¼ë“œ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ë©”ì¸ í•¨ìˆ˜ì—ì„œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ë° ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    print("ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ê°€ì ¸ì˜¤ê¸° ì‹œì‘!")
    start_scheduler()
    server_thread = Thread(target=run_server)
    server_thread.start()
