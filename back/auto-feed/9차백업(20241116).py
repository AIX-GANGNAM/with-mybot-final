# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import warnings
import urllib3
from urllib3.exceptions import InsecureRequestWarning
import logging

# SSL ê²½ê³  ë¬´ì‹œ ì„¤ì •
warnings.simplefilter('ignore', InsecureRequestWarning)
urllib3.disable_warnings(InsecureRequestWarning)

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
import random
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import openai
import os
from pytrends.request import TrendReq
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.events import EVENT_JOB_ERROR, EVENT_JOB_EXECUTED
import atexit
from firebase_admin import credentials, firestore, initialize_app
import firebase_admin
import dotenv
from fastapi import FastAPI, HTTPException
from threading import Thread
import uvicorn
from pydantic import BaseModel
import requests
from fastapi.middleware.cors import CORSMiddleware
from datetime import datetime, timezone
from google.cloud import storage
import uuid
import re
from fastapi.responses import JSONResponse

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ì•± ìƒì„±
app = FastAPI()

# ì‹œì‘ ì‹œ ì‹¤í–‰ë  ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬(fastapi ì‹¤í–‰í• ë•Œ..)
# ì „ì—­ ë³€ìˆ˜ë¡œ ìŠ¤ì¼€ì¤„ëŸ¬ ì„ ì–¸
scheduler = None

@app.on_event("startup")
async def startup_event():
    global scheduler
    print("ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ê°€ì ¸ì˜¤ê¸° ì‹œì‘!")
    scheduler = start_scheduler()

@app.on_event("shutdown")
async def shutdown_event():
    global scheduler
    print("ì„œë²„ ì¢…ë£Œ...")
    if scheduler:
        scheduler.shutdown()  # ì„œë²„ ì¢…ë£Œ ì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ë„ ì •ìƒ ì¢…ë£Œ

# CORS ì„¤ì • ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# .env íŒŒì¼ ë¡œë“œ
dotenv.load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = "./mybot.json"

# Firebase ì´ˆê¸°í™”
if not firebase_admin._apps:
    cred = credentials.Certificate('./mybot.json')
    initialize_app(cred)

db = firestore.client()

# Firebase Storage í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
storage_client = storage.Client()

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ ë¡œë“œ
model_name = "gogamza/kobart-summarization"
TOKENIZER = AutoTokenizer.from_pretrained(model_name)
MODEL = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# pytrends ì´ˆê¸°í™” ë° íŠ¸ë Œë“œ ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜
def fetch_trending_keywords(id, parentNick, userId):
    pytrends = TrendReq(hl='ko', tz=540)
    trending_searches_df = pytrends.trending_searches(pn='south_korea')
    top_keywords = trending_searches_df[0].tolist()

    # íŠ¸ë Œë“œ ëª©ë¡ ì¶œë ¥
    print("ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸(1~20ë“±) â–¼")
    for i, keyword in enumerate(top_keywords, 1):
        print(f"{i}. {keyword}")

    # Firebaseì—ì„œ í˜„ì¬ ì‚¬ìš©ìì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    users_ref = db.collection('users').document(userId)
    user_doc = users_ref.get()
    found_match = False

    if user_doc.exists:
        user_data = user_doc.to_dict()
        personas = user_data.get('persona', [])  # persona ë°°ì—´ ê°€ì ¸ì˜¤ê¸°

        # ê° í˜ë¥´ì†Œë‚˜ì— ëŒ€í•´ ê²€ì‚¬
        for persona_data in personas:
            # interests ë°°ì—´ì´ ìˆëŠ”ì§€ í™•ì¸
            interests = persona_data.get('INTERESTS', [])
            persona_name = persona_data.get('DPNAME', 'unknown')
            persona_type = persona_data.get('Name', '').lower()  # Joy -> joy

            # í˜ë¥´ì†Œë‚˜ì˜ ê´€ì‹¬ì‚¬ì™€ ì¼ì¹˜í•˜ëŠ” í‚¤ì›Œë“œ ì°¾ê¸°
            matched_keywords = [keyword for keyword in top_keywords 
                              if any(interest.lower() in keyword.lower() for interest in interests)]

            if matched_keywords:
                found_match = True
                for keyword in matched_keywords:
                    # persona_id ìƒì„± (ì˜ˆ: user_joy)
                    persona_id = f"{persona_type}"
                    print(f"\n{persona_name}ì˜ ê´€ì‹¬ì‚¬ì™€ ì¼ì¹˜í•˜ëŠ” í‚¤ì›Œë“œ ë°œê²¬ : {keyword}", ", persona_id: " + persona_id, ", userId: " + userId)
                    crawl_article(
                        f"https://search.naver.com/search.naver?where=news&query={keyword}",
                        keyword,
                        persona_name,
                        id,
                        parentNick,
                        userId,
                        persona_id
                    )

    return {"found_match": found_match, "message": "ê´€ì‹¬ì‚¬ì™€ ì¼ì¹˜í•˜ëŠ” ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ê°€ ì—†ìŠµë‹ˆë‹¤."}

# ê¸°ì‚¬ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_article(url, keyword, persona_name, id, parentNick, userId, persona_id):
    print(f"í¬ë¡¤ë§ ì‹œì‘: {url}")

    chrome_options = Options()
    chrome_options.add_argument("--headless=new")  # ìƒˆë¡œìš´ headless ëª¨ë“œ ì‚¬ìš©
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--ignore-certificate-errors")
    chrome_options.add_argument("--window-size=1920x1080")
    chrome_options.add_argument("--log-level=3")
    chrome_options.add_argument("--silent")
    
    # WebGL ê´€ë ¨ ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•œ ì˜µì…˜ ì¶”ê°€
    chrome_options.add_argument("--disable-software-rasterizer")
    chrome_options.add_argument("--disable-webgl")
    chrome_options.add_argument("--disable-webgl2")
    
    # í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¶”ê°€
    chrome_options.add_argument("--page-load-strategy=none")
    chrome_options.page_load_strategy = 'none'
    
    # User-Agent ì¶”ê°€
    chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)

    try:
        # URL ì ‘ê·¼ ë° í˜ì´ì§€ ë¡œë“œ
        driver.get(url)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'a')))
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        keywords = keyword.split()

        # 1ìˆœìœ„: ì œëª©ì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê¸°ì‚¬ ì°¾ê¸°
        articles = soup.find_all('a', class_='news_tit')

        # 1-1. ì›ë³¸ í‚¤ì›Œë“œë¡œ ë¨¼ì € ì‹œë„
        primary_article = next((link for link in articles if keyword in link.get_text()), None)

        # 1-2. ì‹¤íŒ¨ì‹œ ë¶„ë¦¬ëœ í‚¤ì›Œë“œë¡œ ì‹œë„
        if not primary_article:
            # í‚¤ì›Œë“œ ë¶„ë¦¬
            split_keywords = re.findall('[ê°€-í£]+|\d+', keyword)
            primary_article = next((link for link in articles 
                                if any(kw in link.get_text() for kw in split_keywords)), None)

        if primary_article:
            link = primary_article['href']
            print(f"1ìˆœìœ„ë¡œ ì„ íƒëœ ê¸°ì‚¬ ë§í¬: {link}")
        else:
            # 2ìˆœìœ„: ë³¸ë¬¸ì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê¸°ì‚¬ ì°¾ê¸°
            link = None
            for article in articles:
                article_url = article['href']
                driver.get(article_url)
                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
                article_soup = BeautifulSoup(driver.page_source, 'html.parser')

                # ë³¸ë¬¸ì—ì„œ í‚¤ì›Œë“œ ì°¾ê¸°
                paragraphs = article_soup.find_all(['p', 'div', 'span'], class_=lambda x: x and ('content' in x or 'article' in x))
                content = "\n".join([para.get_text().strip() for para in paragraphs if para.get_text().strip()])
                
                # 2-1. ì›ë³¸ í‚¤ì›Œë“œë¡œ ë¨¼ì € ì‹œë„
                if keyword in content:
                    link = article_url
                    print(f"2ìˆœìœ„ë¡œ ì„ íƒëœ ê¸°ì‚¬ ë§í¬: {link}")
                    break
                
                # 2-2. ì‹¤íŒ¨ì‹œ ë¶„ë¦¬ëœ í‚¤ì›Œë“œë¡œ ì‹œë„
                split_keywords = re.findall('[ê°€-í£]+|\d+', keyword)
                if any(kw in content for kw in split_keywords):
                    link = article_url
                    print(f"2ìˆœìœ„ë¡œ ì„ íƒëœ ê¸°ì‚¬ ë§í¬: {link}")
                    break

        # ê¸°ì‚¬ë¥¼ ì°¾ì€ ê²½ìš° ë³¸ë¬¸ê³¼ ì´ë¯¸ì§€ ì¶”ì¶œ
        if link:
            driver.get(link)
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'img')))
            article_soup = BeautifulSoup(driver.page_source, 'html.parser')

            # ë³¸ë¬¸ ì¶”ì¶œ ë¡œì§ ê°œì„ 
            content = None
            
            # 1. í° ì»¨í…Œì´ë„ˆì—ì„œ ë³¸ë¬¸ ì°¾ê¸°
            main_contents = article_soup.find_all(['div', 'article'], 
                class_=lambda x: x and ('article' in str(x).lower() or 'content' in str(x).lower()))
            
            for main_content in main_contents:
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for unwanted in main_content.find_all(['script', 'style', 'iframe', 'a']):
                    unwanted.decompose()
                
                paragraphs = main_content.find_all(['p', 'div', 'span'])
                text = ' '.join([p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > 30])
                
                if len(text) > 200:  # ì˜ë¯¸ ìˆëŠ” ê¸¸ì´ì˜ í…ìŠ¤íŠ¸ì¸ ê²½ìš°
                    content = text
                    break

            # 2. ê°œë³„ ë¬¸ë‹¨ì—ì„œ ì°¾ê¸° (1ë²ˆì´ ì‹¤íŒ¨í•œ ê²½ìš°)
            if not content:
                paragraphs = article_soup.find_all(['p', 'div', 'span'], 
                    class_=lambda x: x and ('article' in str(x).lower() or 'content' in str(x).lower()))
                content = ' '.join([para.get_text().strip() for para in paragraphs 
                                  if len(para.get_text().strip()) > 30])

            if content:
                # ì¤‘ë³µ ê³µë°± ì œê±° ë° ì¤„ë°”ê¿ˆ ì •ë¦¬
                content = ' '.join([line.strip() for line in content.splitlines() if line.strip()])
                print("ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ")
            else:
                print("ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨")
                content = None

            # ì´ë¯¸ì§€ URL ì¶”ì¶œ (ìƒˆë¡œìš´ í•¨ìˆ˜ ì‚¬ìš©)
            image_url = find_main_image(article_soup, link)
            if not image_url:
                print("ë©”ì¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                image_url = "https://example.com/default-image.jpg"
            else:
                print(f"ì°¾ì€ ë©”ì¸ ì´ë¯¸ì§€ URL: {image_url}")
            
            # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬: ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if image_url:
                if image_url.startswith('//'):
                    image_url = 'https:' + image_url
                elif not image_url.startswith('http'):
                    base_url = url.split('/')[0:3]
                    base_url = '/'.join(base_url)
                    image_url = base_url + image_url if image_url.startswith('/') else base_url + '/' + image_url

            # ìœ íš¨í•œ ì´ë¯¸ì§€ URLì¸ì§€ í™•ì¸
            if not image_url or not image_url.startswith('http'):
                image_url = "https://example.com/default-image.jpg"

            print(f"ìµœì¢… ì´ë¯¸ì§€ URL: {image_url}")

            image_url = upload_image_to_storage(image_url, 'mirrorgram-20713.appspot.com', f'feeds/{generate_uuid()}.jpg')


        else:
            print("ì œëª©ê³¼ ë³¸ë¬¸ì— í‚¤ì›Œë“œ ì¡°ê±´ì„ ì¶©ì¡±í•˜ëŠ” ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            content = None
            image_url = None

    except Exception as e:
        print(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        content = None
        image_url = None

    finally:
        driver.quit()

    # ê¸°ì‚¬ ìš”ì•½ ë° í˜ë¥´ì†Œë‚˜ í”¼ë“œ ìƒì„±
    if content:
        # ì—¬ê¸°ì„œ ìƒˆë¡œìš´ UUID ìƒì„±
        new_id = generate_uuid()
        summarize_and_create_feed(content, image_url, persona_name, new_id, parentNick, userId, persona_id)
    else:
        print("ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")


# Firebase Storageì— ì´ë¯¸ì§€ ì—…ë¡œë“œ í•¨ìˆ˜ ì •ì˜
def upload_image_to_storage(image_url, bucket_name, destination_blob_name):
    try:
        # requests ì„¸ì…˜ ìƒì„± ë° í—¤ë” ì„¤ì •
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
            'Accept-Language': 'ko,en;q=0.9,en-US;q=0.8',
            'Referer': 'https://www.google.com/'
        }

        # Firebase Storage ë²„í‚· ì´ˆê¸°í™”
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(destination_blob_name)

        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œë„ (SSL ê²€ì¦ ë¹„í™œì„±í™”)
        try:
            response = requests.get(image_url, headers=headers, verify=False, timeout=10)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"ì²« ë²ˆì§¸ ì‹œë„ ì‹¤íŒ¨: {e}")
            
            # ë‹¤ë¥¸ User-Agentë¡œ ì¬ì‹œë„
            headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            try:
                response = requests.get(image_url, headers=headers, verify=False, timeout=10)
                response.raise_for_status()
            except requests.exceptions.RequestException as e:
                print(f"ë‘ ë²ˆì§¸ ì‹œë„ ì‹¤íŒ¨: {e}")
                return "https://example.com/default-image.jpg"

        # Content-Type í™•ì¸ ë° ì„¤ì •
        content_type = response.headers.get('Content-Type', 'image/jpeg')
        if 'image' not in content_type:
            content_type = 'image/jpeg'  # ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •

        # ì´ë¯¸ì§€ ë°ì´í„°ê°€ ìœ íš¨í•œì§€ í™•ì¸
        if len(response.content) < 100:  # ë„ˆë¬´ ì‘ì€ íŒŒì¼ì€ ì œì™¸
            print("ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë¯¸ì§€ ë°ì´í„°")
            return "https://example.com/default-image.jpg"

        # Firebase Storageì— ì—…ë¡œë“œ
        blob.upload_from_string(
            response.content,
            content_type=content_type
        )

        # ì—…ë¡œë“œëœ ì´ë¯¸ì§€ì— ê³µê°œ ê¶Œí•œ ë¶€ì—¬
        blob.make_public()

        # ìºì‹œ ì œì–´ ì„¤ì •
        blob.cache_control = 'public, max-age=3600'
        blob.patch()

        return blob.public_url

    except Exception as e:
        print(f"Firebase Storageì— ì´ë¯¸ì§€ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # ìƒì„¸í•œ ì—ëŸ¬ ì •ë³´ ì¶œë ¥
        if isinstance(e, requests.exceptions.RequestException):
            print(f"Request ì—ëŸ¬ ìƒì„¸: {e.response.status_code if hasattr(e, 'response') else 'No status code'}")
            print(f"Request ì—ëŸ¬ í—¤ë”: {e.response.headers if hasattr(e, 'response') else 'No headers'}")
        
        return "https://example.com/default-image.jpg"

# ê¸°ì‚¬ ìš”ì•½ ë° í˜ë¥´ì†Œë‚˜ í”¼ë“œ ìƒì„± í•¨ìˆ˜
def summarize_and_create_feed(content, image_url, persona_name, id, parentNick, userId, persona_id):
    try:
        global TOKENIZER, MODEL
        summarized_text = enhanced_summarize(content, MODEL, TOKENIZER)

        if not summarized_text:
            print("ìš”ì•½ëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        openai.api_key = os.getenv("OPENAI_API_KEY")
        if not openai.api_key:
            print("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        # ìˆ˜ì •ëœ í˜ë¥´ì†Œë‚˜ ì„¤ëª…
        persona_descriptions = {
            "joy": "í•­ìƒ ê¸ì •ì ì´ê³  ë‚™ê´€ì ì¸ ì‹œê°ìœ¼ë¡œ ì„¸ìƒì„ ë°”ë¼ë³´ë©°, ëª¨ë“  ì¼ì—ì„œ ì¢‹ì€ ì ì„ ì°¾ìœ¼ë ¤ ë…¸ë ¥í•©ë‹ˆë‹¤.",
            "anger": "ë¶ˆì˜ì— ëŒ€í•´ ì°¸ì§€ ëª»í•˜ê³  ì‰½ê²Œ í™”ë¥¼ ë‚´ë©°, ì§ì„¤ì ì´ê³  ê³µê²©ì ì¸ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ê³¤ í•©ë‹ˆë‹¤.",
            "sadness": "ì„¸ìƒì˜ ì–´ë‘ìš´ ë©´ì„ ìì£¼ ë³´ë©°, ì‘ì€ ì¼ì—ë„ ì‰½ê²Œ ìš°ìš¸í•´ì§€ê³  ëˆˆë¬¼ì„ í˜ë¦¬ê³¤ í•©ë‹ˆë‹¤.",
            "custom": "ë§ˆë™ì„ì²˜ëŸ¼ ì¹œê·¼í•˜ê³  ë“¬ì§í•œ ë§íˆ¬ë¡œ ì´ì•¼ê¸°í•˜ë©°, ì¢…ì¢… 'ì•½ì†í•´? ì§€ì¼œ!' ê°™ì€ ì‹œê·¸ë‹ˆì²˜ ë©˜íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì–´ë ¤ìš´ ìƒí™©ì—ì„œë„ ìœ ë¨¸ëŸ¬ìŠ¤í•˜ê²Œ ëŒ€ì²˜í•˜ëŠ” ìŠ¤íƒ€ì¼ì…ë‹ˆë‹¤."
        }

        emoji_map = {
            "joy": "ğŸ˜ŠğŸ‘",
            "anger": "ğŸ˜¡ğŸ‘Š",
            "sadness": "ğŸ˜¢ğŸ’”",
            "custom": "ğŸ’ªğŸ˜"
        }

        temperature_map = {
            "joy": 0.9,
            "anger": 0.8,
            "sadness": 0.7,
            "custom": 0.8
        }

        # í˜ë¥´ì†Œë‚˜ í”„ë¡œí•„ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
        persona_profile_image_url = get_persona_profile_image(userId, persona_id)
        print("persona_profile_image_url: " + persona_profile_image_url)
        
        # persona_idë¥¼ ì§ì ‘ ì‚¬ìš© (joy, anger, sadness, custom)
        if persona_id not in persona_descriptions:
            print(f"ì•Œ ìˆ˜ ì—†ëŠ” í˜ë¥´ì†Œë‚˜: {persona_id}")
            return

        selected_emoji = emoji_map.get(persona_id, "")
        selected_temperature = temperature_map.get(persona_id, 0.7)

        persona_prompt = (
            f"ê¸°ì‚¬ ìš”ì•½: \n{summarized_text}\n\n"
            f"ë‹¹ì‹ ì€ '{persona_id}'ë¼ëŠ” ìºë¦­í„°ì…ë‹ˆë‹¤. {persona_descriptions[persona_id]} "
            f"ì´ëŸ° ì„±ê²©ì˜ ìºë¦­í„°ê°€ ìœ„ ê¸°ì‚¬ë¥¼ ì½ê³  ê¸€ì„ ì‘ì„±í•œë‹¤ê³  ìƒê°í•˜ê³ , "
            f"ì´ëª¨í‹°ì½˜({selected_emoji})ì„ í¬í•¨í•´ 300ì ë‚´ì™¸ë¡œ ì˜ê²¬ì´ ì•ˆ ëŠê¸°ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”. "
            f"ë¶ˆí•„ìš”í•œ ë§ì€ í•˜ì§€ ë§ê³  í•µì‹¬ë§Œ ë§í•´ì£¼ì„¸ìš”."
        )

        response = openai.ChatCompletion.create(
            model="gpt-4o-mini", #gpt-3.5-turbo gpt-4o-mini
            messages=[
                {"role": "user", "content": persona_prompt}
            ],
            temperature=selected_temperature
        )

        # í˜„ì¬ UTC ì‹œê°„ì— íƒ€ì„ì¡´ ì •ë³´ ì¶”ê°€ í›„ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        created_at = datetime.now(timezone.utc).isoformat()

        if response and response.choices:
            persona_feed = response['choices'][0]['message']['content'].strip()
            print(f"\n{persona_name}ì˜ í”¼ë“œ:\n{persona_feed}")

            # Firestoreì— í”¼ë“œ ì €ì¥
            feed_data = {
                "id" : id,
                "caption": persona_feed,
                "image": image_url,
                "nick": f"{parentNick}ì˜ {persona_name}",
                "userId": userId,
                "createdAt": created_at,  # ISO í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ ì €ì¥
                "likes": [],
                "comments": [],
                "subCommentId": [],
                "personaprofileImage": persona_profile_image_url,  # í˜ë¥´ì†Œë‚˜ í”„ë¡œí•„ ì´ë¯¸ì§€ URL
            }
            # print(f"\nFirestoreì— ì €ì¥í•  í”¼ë“œ ë°ì´í„°: {feed_data}")
            
            # í”¼ë“œ ë°ì´í„° ì €ì¥
            try:
                # Firestoreì— ë°ì´í„° ì¶”ê°€
                feed_ref = db.collection("feeds").document(id)  # ì§€ì •í•œ idë¡œ ë¬¸ì„œ ìƒì„±
                feed_ref.set(feed_data)
                print(f"\n{persona_name}ì˜ í”¼ë“œê°€ Firestoreì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ë¬¸ì„œ ID: {id}")

                # ì €ì¥ëœ ë¬¸ì„œì˜ ID ê°€ì ¸ì˜¤ê¸°
                feed_ref = db.collection("feeds").document(id)
                feed = feed_ref.get()
                if feed.exists:
                    print("Firestoreì— ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:", feed.to_dict())
                else:
                    print("Firestoreì— ë°ì´í„°ê°€ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"Firestoreì— í”¼ë“œë¥¼ ì €ì¥í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

        else:
            print(f"\n{persona_name}ì˜ í”¼ë“œ: OpenAI API ì‘ë‹µì—ì„œ í”¼ë“œ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        if image_url:
            print(f"ê¸°ì‚¬ ì´ë¯¸ì§€: {image_url}")

    except Exception as e:
        print(f"í”¼ë“œ ì‘ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# UUID ìƒì„± í•¨ìˆ˜
def generate_uuid():
    return str(uuid.uuid4())

def chunk_text(text, max_chunk_size=1000):
    """
    ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ìˆëŠ” ì²­í¬ë¡œ ë¶„í• 
    """
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    sentences = text.split('.')
    chunks = []
    current_chunk = []
    current_length = 0
    
    for sentence in sentences:
        sentence = sentence.strip() + '.'
        sentence_length = len(sentence)
        
        if current_length + sentence_length > max_chunk_size:
            if current_chunk:
                chunks.append(' '.join(current_chunk))
            current_chunk = [sentence]
            current_length = sentence_length
        else:
            current_chunk.append(sentence)
            current_length += sentence_length
    
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    
    return chunks

def enhanced_summarize(content, model, tokenizer):
    """
    í–¥ìƒëœ í…ìŠ¤íŠ¸ ìš”ì•½ í•¨ìˆ˜
    """
    try:
        # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        chunks = chunk_text(content)
        summaries = []
        
        for chunk in chunks:
            # ê° ì²­í¬ì— ëŒ€í•´ ìš”ì•½ ìˆ˜í–‰
            inputs = tokenizer([chunk], max_length=1024, return_tensors="pt", truncation=True)
            
            # ìš”ì•½ ìƒì„± íŒŒë¼ë¯¸í„° ì¡°ì •
            summary_ids = model.generate(
                inputs["input_ids"],
                max_length=150,          # ìµœëŒ€ ìš”ì•½ ê¸¸ì´
                min_length=50,           # ìµœì†Œ ìš”ì•½ ê¸¸ì´
                length_penalty=2.0,      # ê¸¸ì´ í˜ë„í‹° (ë†’ì„ìˆ˜ë¡ ë” ê¸´ ìš”ì•½ ìƒì„±)
                num_beams=4,            # ë¹” ì„œì¹˜ í¬ê¸°
                early_stopping=True,     # ì¡°ê¸° ì¢…ë£Œ
                no_repeat_ngram_size=2,  # ë°˜ë³µ êµ¬ë¬¸ ë°©ì§€
                use_cache=True          # ìºì‹œ ì‚¬ìš©ìœ¼ë¡œ ì†ë„ í–¥ìƒ
            )
            
            chunk_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
            summaries.append(chunk_summary)
        
        # ì „ì²´ ìš”ì•½ í†µí•©
        if len(summaries) > 1:
            # ì—¬ëŸ¬ ì²­í¬ì˜ ìš”ì•½ì„ í•˜ë‚˜ë¡œ í†µí•©
            final_summary = " ".join(summaries)
            
            # í†µí•©ëœ ìš”ì•½ì— ëŒ€í•´ í•œ ë²ˆ ë” ìš”ì•½ ìˆ˜í–‰
            inputs = tokenizer([final_summary], max_length=1024, return_tensors="pt", truncation=True)
            summary_ids = model.generate(
                inputs["input_ids"],
                max_length=200,
                min_length=100,
                length_penalty=2.0,
                num_beams=4,
                early_stopping=True,
                no_repeat_ngram_size=2
            )
            final_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        else:
            final_summary = summaries[0]
        
        return final_summary
    
    except Exception as e:
        print(f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return content[:500] + "..."  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì•ë¶€ë¶„ë§Œ ë°˜í™˜

# Firebaseì—ì„œ í˜ë¥´ì†Œë‚˜ í”„ë¡œí•„ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_persona_profile_image(user_id, persona_type):
    try:
        user_ref = db.collection("users").document(user_id)
        user_doc = user_ref.get()
        
        if not user_doc.exists:
            print(f"ì‚¬ìš©ì ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {user_id}")
            return "https://example.com/default-persona-image.jpg"
            
        user_data = user_doc.to_dict()
        
        # persona ë°°ì—´ì—ì„œ í•´ë‹¹ íƒ€ì…ì˜ í˜ë¥´ì†Œë‚˜ ì°¾ê¸°
        matching_persona = next(
            (p for p in user_data.get('persona', []) if p.get('Name', '').lower() == persona_type.lower()),
            None
        )
        
        if matching_persona:
            profile_image = matching_persona.get('IMG')
            if profile_image:
                return profile_image
                
        print(f"í˜ë¥´ì†Œë‚˜ í”„ë¡œí•„ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {persona_type}")
        return "https://example.com/default-persona-image.jpg"
        
    except Exception as e:
        print(f"í”„ë¡œí•„ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
        return "https://example.com/default-persona-image.jpg"

# ìë™ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€[ì „ì²´ ì‹¤í–‰]
@app.post("/feedAutomatic")
async def feedAutomatic(feed_data: dict):
    try:
        id = feed_data.get("id")
        parentNick = feed_data.get("parentNick")
        userId = feed_data.get("userId")

        result = fetch_trending_keywords(id, parentNick, userId)
        
        # ë§¤ì¹˜ë˜ëŠ” í‚¤ì›Œë“œê°€ ì—†ì„ ë•Œ ë©”ì‹œì§€ë¥¼ ë°˜í™˜
        if not result["found_match"]:
            return JSONResponse(status_code=200, content={"message": result["message"]})

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ëª©ë¡ì„ ë°˜í™˜í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸
@app.get("/trendingKeywords")
async def get_trending_keywords():
    try:
        # pytrends ì´ˆê¸°í™” ë° ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ê°€ì ¸ì˜¤ê¸°
        pytrends = TrendReq(hl='ko', tz=540)
        trending_searches_df = pytrends.trending_searches(pn='south_korea')
        top_keywords = trending_searches_df[0].tolist()
        print("ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ :", top_keywords)  # ë¦¬ìŠ¤íŠ¸ íƒ€ì…ìœ¼ë¡œ ì¶œë ¥

        # ê²€ìƒ‰ì–´ ëª©ë¡ ë°˜í™˜
        return {"trending_keywords": top_keywords}

    except Exception as e:
        print(f"ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# ì‚¬ìš©ìê°€ ì„ íƒí•œ í‚¤ì›Œë“œë¡œ ìš”ì•½ëœ í”¼ë“œë¥¼ ìë™ ìƒì„±í•˜ì—¬ Firebaseì— ì €ì¥í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸
@app.post("/generateFeed")
async def generate_feed(request: dict):
    keyword = request.get("keyword")
    persona_type = request.get("persona_type")
    parentNick = request.get("parentNick")
    userId = request.get("userId")
    title = request.get("title")

    print(f"í‚¤ì›Œë“œ: {keyword}, í˜ë¥´ì†Œë‚˜ íƒ€ì…: {persona_type}, parentNick: {parentNick}, userId: {userId}, title: {title}")

    # ìš”ì²­ ê²€ì¦
    if not keyword or not persona_type or not parentNick or not userId:
        raise HTTPException(status_code=400, detail="í‚¤ì›Œë“œ, í˜ë¥´ì†Œë‚˜ íƒ€ì…, parentNick, userIdë¥¼ ëª¨ë‘ ì…ë ¥í•˜ì„¸ìš”.")

    try:
        # ìœ ë‹ˆí¬ ID ìƒì„± ë° í˜ë¥´ì†Œë‚˜ ì´ë¦„ ì„¤ì •
        id = generate_uuid()
        persona_name = title if title else f"{persona_type.capitalize()}"
        
        print(f"í˜ë¥´ì†Œë‚˜ ì´ë¦„: {persona_name}")
        # ê¸°ì‚¬ í¬ë¡¤ë§ ë° ìš”ì•½ ìƒì„± í›„ Firebaseì— ì €ì¥
        article_url = f"https://search.naver.com/search.naver?where=news&query={keyword}"
        crawl_article(article_url, keyword, persona_name, id, parentNick, userId, persona_type)

        return {"message": f"'{keyword}' í‚¤ì›Œë“œì— ëŒ€í•œ '{persona_type}' í˜ë¥´ì†Œë‚˜ë¡œ í”¼ë“œê°€ ìë™ ìƒì„±ë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def find_main_image(soup, article_url):
    """
    ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ë©”ì¸ ì´ë¯¸ì§€ë¥¼ ì°¾ëŠ” í–¥ìƒëœ í•¨ìˆ˜
    """
    try:
        def normalize_url(src, article_url):
            if not src:
                return None
            from urllib.parse import urljoin
            # í”„ë¡œí† ì½œ ìƒëŒ€ URL ì²˜ë¦¬
            if src.startswith('//'):
                return 'https:' + src
            # ì ˆëŒ€ URLì¸ ê²½ìš°
            if src.startswith(('http://', 'https://')):
                return src
            # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
            return urljoin(article_url, src)

        # ì œì™¸í•  ì´ë¯¸ì§€ íŒ¨í„´
        exclude_patterns = [
            'logo', 'banner', 'ad', 'icon', 'button', 'share', 
            'reporter', 'profile', 'thumbnail', 'small', 'nav',
            'footer', 'header', 'email'
        ]

        def is_news_image(img):
            """ë‰´ìŠ¤ ì´ë¯¸ì§€ì¸ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
            # ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ì†ì„± í™•ì¸
            possible_src_attrs = ['src', 'data-src', 'data-original', 'data-lazy-src']
            img_src = None
            for attr in possible_src_attrs:
                if img.get(attr):
                    img_src = img[attr]
                    break
            
            if not img_src:
                # srcset í™•ì¸
                srcset = img.get('srcset', '')
                if srcset:
                    # srcsetì—ì„œ ê°€ì¥ í° ì´ë¯¸ì§€ URL ì¶”ì¶œ
                    srcset_urls = [s.strip().split(' ')[0] for s in srcset.split(',')]
                    if srcset_urls:
                        img_src = srcset_urls[-1]  # ì¼ë°˜ì ìœ¼ë¡œ ë§ˆì§€ë§‰ URLì´ ê°€ì¥ í° ì´ë¯¸ì§€

            if not img_src:
                return False, None

            alt = img.get('alt', '').lower()
            class_name = ' '.join(img.get('class', [])).lower() if img.get('class') else ''
            
            # ì œì™¸ íŒ¨í„´ ì²´í¬
            if any(pattern in img_src.lower() or pattern in alt or pattern in class_name 
                  for pattern in exclude_patterns):
                return False, None

            # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ì´ë¯¸ì§€ íŠ¹ì„± ì²´í¬
            is_news_photo = (
                'photo' in img_src.lower() or
                'image' in img_src.lower() or
                'news' in img_src.lower() or
                any(ext in img_src.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif']) or
                'pictures' in img_src.lower()
            )

            # ì´ë¯¸ì§€ í¬ê¸°ë‚˜ í’ˆì§ˆ ê´€ë ¨ ì†ì„± ì²´í¬
            width = img.get('width', '0')
            height = img.get('height', '0')
            if width.isdigit() and height.isdigit():
                size_ok = int(width) >= 200 and int(height) >= 200
            else:
                size_ok = True

            return is_news_photo and size_ok, img_src

        # 1. ë©”ì¸ ì´ë¯¸ì§€ ì°¾ê¸° (ì¼ë°˜ì ì¸ ì†ì„±/ìœ„ì¹˜ ê¸°ë°˜)
        main_candidates = []
        
        # íŠ¹ì • í´ë˜ìŠ¤ë‚˜ IDë¥¼ ê°€ì§„ ì»¨í…Œì´ë„ˆ ë¨¼ì € í™•ì¸
        main_containers = soup.find_all(['div', 'figure'], class_=lambda x: x and any(keyword in str(x).lower() 
            for keyword in ['article-img', 'news-img', 'image-area', 'photo', 'figure']))
        
        for container in main_containers:
            img = container.find('img')
            if img:
                is_valid, src = is_news_image(img)
                if is_valid and src:
                    score = 10  # ì»¨í…Œì´ë„ˆì—ì„œ ì°¾ì€ ì´ë¯¸ì§€ì— ë†’ì€ ì ìˆ˜ ë¶€ì—¬
                    main_candidates.append((img, score, src))

        # ì¼ë°˜ì ì¸ ì´ë¯¸ì§€ ê²€ìƒ‰
        for img in soup.find_all('img'):
            is_valid, src = is_news_image(img)
            if not is_valid:
                continue

            score = 0
            alt = img.get('alt', '').lower()
            
            # ì ìˆ˜ ë¶€ì—¬ ê¸°ì¤€
            if 'main' in str(img) or 'article' in str(img):
                score += 3
            if len(alt) > 10:  # ì˜ë¯¸ ìˆëŠ” alt í…ìŠ¤íŠ¸
                score += 2
            if img.parent and ('article' in str(img.parent) or 'content' in str(img.parent)):
                score += 2
            
            # ì¶”ê°€ ì ìˆ˜ ë¶€ì—¬: ë³¸ë¬¸ ì˜ì—­ì— ìˆëŠ” ì´ë¯¸ì§€
            article_content = soup.find(['article', 'div'], class_=lambda x: x and 'article' in x.lower())
            if article_content and img in article_content.find_all('img'):
                score += 3

            main_candidates.append((img, score, src))

        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
        main_candidates.sort(key=lambda x: x[1], reverse=True)

        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì´ë¯¸ì§€ ì„ íƒ
        if main_candidates:
            best_img = main_candidates[0][0]
            src = main_candidates[0][2]
            if src:
                normalized_url = normalize_url(src, article_url)
                print(f"ì„ íƒëœ ì´ë¯¸ì§€ (ì ìˆ˜: {main_candidates[0][1]}): {normalized_url}")
                return normalized_url

        # ë°±ì—…: meta íƒœê·¸ì—ì„œ ì´ë¯¸ì§€ ì°¾ê¸°
        meta_img = soup.find('meta', property='og:image')
        if meta_img and meta_img.get('content'):
            return normalize_url(meta_img['content'], article_url)

        return None

    except Exception as e:
        print(f"ì´ë¯¸ì§€ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None

def scheduled_fetch_trending_keywords_for_all_users():
    """ìŠ¤ì¼€ì¤„ëŸ¬ìš© ë˜í¼ í•¨ìˆ˜"""
    try:
        # Firebaseì—ì„œ ëª¨ë“  ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        users_ref = db.collection('users').get()
        
        for user in users_ref:
            user_data = user.to_dict()
            if user_data.get('parentNick'):
                fetch_trending_keywords(
                    id=generate_uuid(),
                    parentNick=user_data['parentNick'],
                    userId=user.id
                )
    except Exception as e:
        logger.error(f"ìŠ¤ì¼€ì¤„ëœ ì‘ì—… ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def job_listener(event):
    """ìŠ¤ì¼€ì¤„ëŸ¬ ì‘ì—… ì‹¤í–‰ ìƒíƒœë¥¼ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ë¦¬ìŠ¤ë„ˆ"""
    if event.exception:
        logger.error('ì‘ì—… ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: %s', str(event.exception))
    else:
        logger.info('ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.')

def start_scheduler():
    """í–¥ìƒëœ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ í•¨ìˆ˜"""
    try:
        scheduler = BackgroundScheduler()
        
        # ì‘ì—… ë¦¬ìŠ¤ë„ˆ ì¶”ê°€
        scheduler.add_listener(job_listener, EVENT_JOB_ERROR | EVENT_JOB_EXECUTED)
        
        # ë§¤ì¼ 0ì‹œì™€ 12ì‹œì— ì‹¤í–‰ë˜ëŠ” ì‘ì—… ì¶”ê°€
        scheduler.add_job(
            scheduled_fetch_trending_keywords_for_all_users,
            'cron',
            hour='6,12,18',  # ì˜¤ì „ 6ì‹œ, ì˜¤í›„ 12ì‹œ, ì˜¤í›„ 6ì‹œ
            id='trending_keywords_job',
            name='Fetch Trending Keywords',
            misfire_grace_time=None,
            coalesce=True,
            max_instances=1,
            # kwargs={
            #     'id': None,
            #     'parentNick': None,
            #     'userId': None
            # }
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
        scheduler.start()
        logger.info("ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ë§¤ì¼ ì˜¤ì „ 6ì‹œ, ì˜¤í›„ 12ì‹œ, ì˜¤í›„ 6ì‹œì— ì‹¤í–‰ë©ë‹ˆë‹¤.")
        
        # í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ ì •ìƒ ì¢…ë£Œ
        def cleanup():
            logger.info("ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤...")
            scheduler.shutdown()
        
        atexit.register(cleanup)
        
        return scheduler
        
    except Exception as e:
        logger.error(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

# FastAPI ì„œë²„ ì‹¤í–‰ ìŠ¤ë ˆë“œ
def run_server():
    uvicorn.run(app, host="0.0.0.0", port=8010)

# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„(python ëª…ë ¹ì–´ë¡œ ì‹¤í–‰í• ë•Œ..!)
if __name__ == "__main__":
    print("ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ê°€ì ¸ì˜¤ê¸° ì‹œì‘!")
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
    scheduler = start_scheduler()
    
    # ì„œë²„ ìŠ¤ë ˆë“œ ì‹œì‘
    server_thread = Thread(target=run_server)
    server_thread.start()