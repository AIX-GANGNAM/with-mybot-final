from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
import random
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import openai
import os
from pytrends.request import TrendReq
from firebase_admin import credentials, firestore, initialize_app
import firebase_admin
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# Firebase ì´ˆê¸°í™”
if not firebase_admin._apps:
    cred = credentials.Certificate('./mybot.json')
    initialize_app(cred)

db = firestore.client()

# pytrends ì´ˆê¸°í™” ë° íŠ¸ë Œë“œ ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜
def fetch_trending_keywords():
    pytrends = TrendReq(hl='ko', tz=540)
    trending_searches_df = pytrends.trending_searches(pn='south_korea')
    top_keywords = trending_searches_df[0].tolist()

    # íŠ¸ë Œë“œ ëª©ë¡ ì¶œë ¥
    print("ì˜¤ëŠ˜ì˜ ëŒ€í•œë¯¼êµ­ íŠ¸ë Œë“œ í‚¤ì›Œë“œ:")
    for i, keyword in enumerate(top_keywords, 1):
        print(f"{i}. {keyword}")

    # Firebaseì—ì„œ í˜ë¥´ì†Œë‚˜ ê´€ì‹¬ì‚¬ ê°€ì ¸ì˜¤ê¸°
    personas_ref = db.collection('personas')
    personas = personas_ref.stream()

    found_match = False
    for persona in personas:
        persona_data = persona.to_dict()
        persona_name = persona_data.get('name')
        interests = persona_data.get('interests', [])

        # í˜ë¥´ì†Œë‚˜ ê´€ì‹¬ì‚¬ì™€ ì¼ì¹˜í•˜ëŠ” í‚¤ì›Œë“œ ì°¾ê¸°
        for interest in interests:
            if any(interest in keyword for keyword in top_keywords):
                found_match = True
                print(f"\n{persona_name}ì˜ ê´€ì‹¬ì‚¬ì™€ ì¼ì¹˜í•˜ëŠ” í‚¤ì›Œë“œ ë°œê²¬: {interest}")
                # í¬ë¡¤ë§í•  ëŒ€ìƒ URL ì„¤ì • (ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€)
                url = f"https://search.naver.com/search.naver?where=news&query={interest}"
                crawl_article(url, interest)

    if not found_match:
        print("ê´€ì‹¬ì‚¬ì™€ ì¼ì¹˜í•˜ëŠ” ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ê¸°ì‚¬ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_article(url, keyword):
    # Chrome ì˜µì…˜ ì„¤ì •
    chrome_options = Options()
    # chrome_options.add_argument("--headless")  # ë³´ì•ˆ ëª¨ë“œì—ì„œ Chromeì„ ì—´ì§€ ì•Šë„ë¡ ì„¤ì •
    chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)

    # Chrome WebDriver ì„¤ì •
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)

    # URL í¬ë¡¤ë§ ë° ê¸°ì‚¬ ë‚´ìš© ë° ì´ë¯¸ì§€ ì¶œë ¥
    try:
        driver.get(url)
        WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, "news_tit")))

        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')

        # ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ í•˜ë‚˜ ì¶œë ¥
        article = soup.find('a', class_='news_tit')
        if article:
            link = article['href']
            print(f"ê¸°ì‚¬ ë§í¬: {link}")

            # ê¸°ì‚¬ ë³¸ë¬¸ ë° ì´ë¯¸ì§€ ì¶œë ¥
            driver.get(link)
            WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            time.sleep(random.uniform(3, 7))

            article_page = driver.page_source
            article_soup = BeautifulSoup(article_page, 'html.parser')

            # ê¸°ì‚¬ ë³¸ë¬¸ ì¶œë ¥
            paragraphs = article_soup.find_all(['p', 'div', 'span'], class_=lambda x: x and ('content' in x or 'article' in x))
            content = "\n".join([para.get_text().strip() for para in paragraphs if para.get_text().strip()])

            # ê¸°ì‚¬ ì´ë¯¸ì§€ ì¶œë ¥
            image_tag = article_soup.find('img')
            image_url = image_tag['src'] if image_tag else None

        else:
            print("ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            content = None
            image_url = None

    finally:
        driver.quit()

    # ê¸°ì‚¬ ë‚´ìš©ì„ ìš”ì•½í•˜ê³  í˜ë¥´ì†Œë‚˜ì˜ ì„±ê²© ë°˜ì˜í•œ í”¼ë“œ ì‘ì„±
    if content:
        summarize_and_create_feed(content, image_url)
    else:
        print("ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")

# ê¸°ì‚¬ ìš”ì•½ ë° í˜ë¥´ì†Œë‚˜ í”¼ë“œ ìƒì„± í•¨ìˆ˜
def summarize_and_create_feed(content, image_url):
    try:
        # KoBART ìš”ì•½ ëª¨ë¸ ë¡œë“œ
        model_name = "gogamza/kobart-summarization"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

        # ê¸°ì‚¬ë¥¼ ìš”ì•½í•˜ì—¬ ê¸¸ì´ë¥¼ ì¤„ì„
        inputs = tokenizer([content], max_length=1024, return_tensors="pt", truncation=True)
        summary_ids = model.generate(inputs["input_ids"], max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)
        summarized_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

        if not summarized_text:
            print("ìš”ì•½ëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        # OpenAI APIë¥¼ ì‚¬ìš©í•´ í˜ë¥´ì†Œë‚˜ ì„±ê²© ë°˜ì˜í•œ í”¼ë“œ ì‘ì„±
        openai.api_key = os.getenv("OPENAI_API_KEY")
        if not openai.api_key:
            print("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        persona_descriptions = {
            "ê¸°ë¯ˆì´": "í•œì¥ êµ¬ì •ì ì´ê³  ë‚©ê´€ì ì¸ ì‹œê°ìœ¼ë¡œ ì„¸ìƒì„ ë¶€ë¥´ë©´, ëª¨ë“  ì¼ì—ì„œ ì¢‹ì€ ì ì„ ì°¾ìœ¼ë ¤ ë…¸ë ¥í•©ë‹ˆë‹¤.",
            "í™”ë‚œì´": "ë¶ˆì˜ì— ëŒ€í•´ ì²´íŒ”ì„ ëª»í•˜ê³  ì‰½ê²Œ í™”ë¥¼ ë‚´ë©´, ì§‘ì„¤ì ì´ê³  ê³µê²©ì ì¸ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ê³ ëŠ” í•©ë‹ˆë‹¤.",
            "ê¹Šì¼ì´": "ëª¨ë“  ê²ƒì— ëŒ€í•´ ë¹„íŒì ì´ê³  ë‡¨ì†Œì ì¸ íƒœë„ë¥¼ ë³´ì´ë©°, ë‚ ì¹˜ë¥¸ ì§€ì ì„ ì¦ê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "ìŠ¬í•­ì´": "ì„¸ìƒì˜ ì–´ë‘¡ì€ ë©”ì¼ì„ ìì£¼ ë°›ìœ¼ë©´, ì†Œë¦¬ë¥¼ ì‰½ê²Œ ì¤„ì–´ì„œ ë†“ê³  ë²Œì–´ì§€ëŠ” í™•ì´ ë§ìŠµë‹ˆë‹¤.",
            "ì„ ë¹„": "í•™ì‹ì´ ë†’ê³  í’í•¨ì´ ë°”ë¥´ë©°, ë…ì„œê°€ ë†’ì€ ì–´ë¥¸ì˜ ë²”êµ¬ì— ìœ ë¦°ì ì¸ ë²”ìœ„ë„ì— ëŒ€í•´ ì´ì–´ì¡ŒìŠµë‹ˆë‹¤.",
        }

        emoji_map = {
            "ê¸°ë¯ˆì´": "ğŸ˜ŠğŸ‘",
            "í™”ë‚œì´": "ğŸ˜¡ğŸ‘Š",
            "ê¹Šì¼ì´": "ğŸ˜’ğŸ™„",
            "ìŠ¬í•­ì´": "ğŸ˜¢ğŸ’”",
            "ì„ ë¹„": "ğŸ¤®ğŸ“š",
        }

        temperature_map = {
            "ê¸°ë¯ˆì´": 0.9,
            "í™”ë‚œì´": 0.8,
            "ê¹Šì¼ì´": 0.7,
            "ìŠ¬í•­ì´": 0.6,
            "ì„ ë¹„": 0.5,
        }

        persona_feeds = []
        for persona in persona_descriptions.keys():
            selected_emoji = emoji_map.get(persona, "")
            selected_temperature = temperature_map.get(persona, 0.7)

            persona_prompt = (
                f"ê¸°ì‚¬ ìš”ì•½: \n{summarized_text}\n\n"
                f"ë‹¹ì‹ ì€ 'ìŠ¤í„°ì¸'ì´ë¼ëŠ” ì—¬ì„±ìœ¼ë¡œ 'êµ¬ë¬¸ì˜'ì•ˆì´í•´ì§€ ê³ ì¥ì´ìš”."
            )

    except Exception as e:
        print(f"í”¼ë“œ ì‘ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    fetch_trending_keywords()
