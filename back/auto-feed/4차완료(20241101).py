# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import warnings
import urllib3
from urllib3.exceptions import InsecureRequestWarning

# SSL ê²½ê³  ë¬´ì‹œ ì„¤ì •
warnings.simplefilter('ignore', InsecureRequestWarning)
urllib3.disable_warnings(InsecureRequestWarning)

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
import random
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import openai
import os
from pytrends.request import TrendReq
from apscheduler.schedulers.background import BackgroundScheduler
import atexit
from firebase_admin import credentials, firestore, initialize_app
import firebase_admin
import dotenv
from fastapi import FastAPI, HTTPException
from threading import Thread
import uvicorn
from pydantic import BaseModel
import requests
from fastapi.middleware.cors import CORSMiddleware
from datetime import datetime, timezone
from google.cloud import storage
import uuid
import re  # ì •ê·œ í‘œí˜„ì‹ ëª¨ë“ˆ ì„í¬íŠ¸ ì¶”ê°€
from fastapi.responses import JSONResponse

# FastAPI ì•± ìƒì„±
app = FastAPI()

# CORS ì„¤ì • ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ëª¨ë“  ì¶œì²˜ í—ˆìš© (í•„ìš”ì— ë”°ë¼ íŠ¹ì • ì¶œì²˜ë¡œ ì œí•œ ê°€ëŠ¥)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# .env íŒŒì¼ ë¡œë“œ
dotenv.load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = "./mybot.json"

# Firebase ì´ˆê¸°í™”
if not firebase_admin._apps:
    cred = credentials.Certificate('./mybot.json')
    initialize_app(cred)

db = firestore.client()

# Firebase Storage í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
storage_client = storage.Client()

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ ë¡œë“œ
model_name = "gogamza/kobart-summarization"
TOKENIZER = AutoTokenizer.from_pretrained(model_name)
MODEL = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# pytrends ì´ˆê¸°í™” ë° íŠ¸ë Œë“œ ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜
def fetch_trending_keywords(id, parentNick, userId):
    pytrends = TrendReq(hl='ko', tz=540)
    trending_searches_df = pytrends.trending_searches(pn='south_korea')
    top_keywords = trending_searches_df[0].tolist()

    # íŠ¸ë Œë“œ ëª©ë¡ ì¶œë ¥
    print("ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸(1~20ë“±) â–¼")
    for i, keyword in enumerate(top_keywords, 1):
        print(f"{i}. {keyword}")

    # Firebaseì—ì„œ í˜ë¥´ì†Œë‚˜ ê´€ì‹¬ì‚¬ ê°€ì ¸ì˜¤ê¸°
    personas_ref = db.collection('personas')
    personas = personas_ref.stream()

    found_match = False

    for persona in personas:
        persona_data = persona.to_dict()
        persona_id = persona.id
        persona_name = persona_data.get('profile', {}).get('name', 'unknown')
        interests = persona_data.get('interests', [])

        # í˜ë¥´ì†Œë‚˜ ê´€ì‹¬ì‚¬ì™€ ì¼ì¹˜í•˜ëŠ” í‚¤ì›Œë“œ ì°¾ê¸°
        matched_keywords = [keyword for keyword in top_keywords if any(interest in keyword for interest in interests)]

        if matched_keywords:
            found_match = True
            for keyword in matched_keywords:
                print(f"\n{persona_name}ì˜ ê´€ì‹¬ì‚¬ì™€ ì¼ì¹˜í•˜ëŠ” í‚¤ì›Œë“œ ë°œê²¬ : {keyword}")
                crawl_article(f"https://search.naver.com/search.naver?where=news&query={keyword}", keyword, persona_name, id, parentNick, userId, persona_id)

    return {"found_match": found_match, "message": "ê´€ì‹¬ì‚¬ì™€ ì¼ì¹˜í•˜ëŠ” ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ê°€ ì—†ìŠµë‹ˆë‹¤."}

# ê¸°ì‚¬ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_article(url, keyword, persona_name, id, parentNick, userId, persona_id):
    print(f"í¬ë¡¤ë§ ì‹œì‘: {url}")
    
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--ignore-certificate-errors")
    chrome_options.add_argument("--window-size=1920x1080")
    chrome_options.add_argument("--log-level=3")  # ì¶”ê°€: ë¶ˆí•„ìš”í•œ ë¡œê·¸ ì œê±°
    chrome_options.add_argument("--silent")  # ì¶”ê°€: ë¡œê·¸ ìµœì†Œí™”
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)

    try:
        # URL ì ‘ê·¼ ë° í˜ì´ì§€ ë¡œë“œ
        driver.get(url)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'a')))
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        keywords = keyword.split()

        # 1ìˆœìœ„: ì œëª©ì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê¸°ì‚¬ ì°¾ê¸°
        articles = soup.find_all('a', class_='news_tit')
        primary_article = next((link for link in articles if any(kw in link.get_text() for kw in keywords)), None)

        if primary_article:
            # 1ìˆœìœ„ ê¸°ì‚¬ ì„ íƒ ì‹œ ë§í¬ ì‚¬ìš©
            link = primary_article['href']
            print(f"1ìˆœìœ„ë¡œ ì„ íƒëœ ê¸°ì‚¬ ë§í¬: {link}")
        else:
            # 2ìˆœìœ„: ë³¸ë¬¸ì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê¸°ì‚¬ ì°¾ê¸°
            link = None
            for article in articles:
                article_url = article['href']
                driver.get(article_url)
                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
                article_soup = BeautifulSoup(driver.page_source, 'html.parser')

                # ë³¸ë¬¸ì—ì„œ í‚¤ì›Œë“œ ì°¾ê¸°
                paragraphs = article_soup.find_all(['p', 'div', 'span'], class_=lambda x: x and ('content' in x or 'article' in x))
                content = "\n".join([para.get_text().strip() for para in paragraphs if para.get_text().strip()])
                
                if any(kw in content for kw in keywords):
                    link = article_url
                    print(f"2ìˆœìœ„ë¡œ ì„ íƒëœ ê¸°ì‚¬ ë§í¬: {link}")
                    break

        # ê¸°ì‚¬ë¥¼ ì°¾ì€ ê²½ìš° ë³¸ë¬¸ê³¼ ì´ë¯¸ì§€ ì¶”ì¶œ
        if link:
            driver.get(link)
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'img')))
            article_soup = BeautifulSoup(driver.page_source, 'html.parser')

            # ë³¸ë¬¸ ì¶”ì¶œ
            paragraphs = article_soup.find_all(['p', 'div', 'span'], class_=lambda x: x and ('content' in x or 'article' in x))
            content = "\n".join([para.get_text().strip() for para in paragraphs if para.get_text().strip()])

            if content:
                # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ ì¤„ì´ê¸°
                content = "\n".join([line.strip() for line in content.splitlines() if line.strip() != ""])
                print("ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ")
            else:
                print("ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨")
                content = None

            # ì´ë¯¸ì§€ URL ì¶”ì¶œ (ìƒˆë¡œìš´ í•¨ìˆ˜ ì‚¬ìš©)
            image_url = find_main_image(article_soup, link)
            if not image_url:
                print("ë©”ì¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                image_url = "https://example.com/default-image.jpg"
            else:
                print(f"ì°¾ì€ ë©”ì¸ ì´ë¯¸ì§€ URL: {image_url}")
            
            # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬: ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if image_url:
                if image_url.startswith('//'):
                    image_url = 'https:' + image_url
                elif not image_url.startswith('http'):
                    base_url = url.split('/')[0:3]
                    base_url = '/'.join(base_url)
                    image_url = base_url + image_url if image_url.startswith('/') else base_url + '/' + image_url

            # ìœ íš¨í•œ ì´ë¯¸ì§€ URLì¸ì§€ í™•ì¸
            if not image_url or not image_url.startswith('http'):
                image_url = "https://example.com/default-image.jpg"

            print(f"ìµœì¢… ì´ë¯¸ì§€ URL: {image_url}")

            image_url = upload_image_to_storage(image_url, 'mirrorgram-20713.appspot.com', f'feeds/{generate_uuid()}.jpg')


        else:
            print("ì œëª©ê³¼ ë³¸ë¬¸ì— í‚¤ì›Œë“œ ì¡°ê±´ì„ ì¶©ì¡±í•˜ëŠ” ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            content = None
            image_url = None

    except Exception as e:
        print(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        content = None
        image_url = None

    finally:
        driver.quit()

    # ê¸°ì‚¬ ìš”ì•½ ë° í˜ë¥´ì†Œë‚˜ í”¼ë“œ ìƒì„±
    if content:
        # ì—¬ê¸°ì„œ ìƒˆë¡œìš´ UUID ìƒì„±
        new_id = generate_uuid()
        summarize_and_create_feed(content, image_url, persona_name, new_id, parentNick, userId, persona_id)
    else:
        print("ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")


# Firebase Storageì— ì´ë¯¸ì§€ ì—…ë¡œë“œ í•¨ìˆ˜ ì •ì˜
def upload_image_to_storage(image_url, bucket_name, destination_blob_name):
    try:
        # requests ì„¸ì…˜ ìƒì„± ë° í—¤ë” ì„¤ì •
        session = requests.Session()
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
            'Accept-Language': 'ko,en;q=0.9,en-US;q=0.8',
            'Referer': 'https://www.google.com/'
        }
        
        # Firebase Storage ë²„í‚· ì´ˆê¸°í™”
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(destination_blob_name)

        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œë„ (ê¸°ë³¸ í—¤ë”)
        try:
            response = session.get(image_url, headers=headers, verify=False, timeout=10)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"ì²« ë²ˆì§¸ ì‹œë„ ì‹¤íŒ¨: {e}")
            
            # ë‹¤ë¥¸ User-Agentë¡œ ì¬ì‹œë„
            headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            try:
                response = session.get(image_url, headers=headers, verify=False, timeout=10)
                response.raise_for_status()
            except requests.exceptions.RequestException as e:
                print(f"ë‘ ë²ˆì§¸ ì‹œë„ ì‹¤íŒ¨: {e}")
                return "https://example.com/default-image.jpg"

        # Content-Type í™•ì¸ ë° ì„¤ì •
        content_type = response.headers.get('Content-Type', 'image/jpeg')
        if 'image' not in content_type:
            content_type = 'image/jpeg'  # ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •

        # ì´ë¯¸ì§€ ë°ì´í„°ê°€ ìœ íš¨í•œì§€ í™•ì¸
        if len(response.content) < 100:  # ë„ˆë¬´ ì‘ì€ íŒŒì¼ì€ ì œì™¸
            print("ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë¯¸ì§€ ë°ì´í„°")
            return "https://example.com/default-image.jpg"

        # Firebase Storageì— ì—…ë¡œë“œ
        blob.upload_from_string(
            response.content,
            content_type=content_type
        )

        # ì—…ë¡œë“œëœ ì´ë¯¸ì§€ì— ê³µê°œ ê¶Œí•œ ë¶€ì—¬
        blob.make_public()

        # ìºì‹œ ì œì–´ ì„¤ì •
        blob.cache_control = 'public, max-age=3600'
        blob.patch()

        return blob.public_url

    except Exception as e:
        print(f"Firebase Storageì— ì´ë¯¸ì§€ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # ìƒì„¸í•œ ì—ëŸ¬ ì •ë³´ ì¶œë ¥
        if isinstance(e, requests.exceptions.RequestException):
            print(f"Request ì—ëŸ¬ ìƒì„¸: {e.response.status_code if hasattr(e, 'response') else 'No status code'}")
            print(f"Request ì—ëŸ¬ í—¤ë”: {e.response.headers if hasattr(e, 'response') else 'No headers'}")
        
        return "https://example.com/default-image.jpg"

# ê¸°ì‚¬ ìš”ì•½ ë° í˜ë¥´ì†Œë‚˜ í”¼ë“œ ìƒì„± í•¨ìˆ˜
def summarize_and_create_feed(content, image_url, persona_name, id, parentNick, userId, persona_id):
    try:
        # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì½”ë“œ ì œê±°
        global TOKENIZER, MODEL
        # ë°”ë¡œ ìš”ì•½ ì‹œì‘
        summarized_text = enhanced_summarize(content, MODEL, TOKENIZER)

        if not summarized_text:
            print("ìš”ì•½ëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        # OpenAI APIë¥¼ ì‚¬ìš©í•´ í˜ë¥´ì†Œë‚˜ ì„±ê²© ë°˜ì˜í•œ í”¼ë“œ ì‘ì„±
        openai.api_key = os.getenv("OPENAI_API_KEY")
        if not openai.api_key:
            print("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        persona_descriptions = {
            "joy": "í•­ìƒ ê¸ì •ì ì´ê³  ë‚™ê´€ì ì¸ ì‹œê°ìœ¼ë¡œ ì„¸ìƒì„ ë°”ë¼ë³´ë©°, ëª¨ë“  ì¼ì—ì„œ ì¢‹ì€ ì ì„ ì°¾ìœ¼ë ¤ ë…¸ë ¥í•©ë‹ˆë‹¤.",
            "anger": "ë¶ˆì˜ì— ëŒ€í•´ ì°¸ì§€ ëª»í•˜ê³  ì‰½ê²Œ í™”ë¥¼ ë‚´ë©°, ì§ì„¤ì ì´ê³  ê³µê²©ì ì¸ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ê³¤ í•©ë‹ˆë‹¤.",
            "disgust": "ëª¨ë“  ê²ƒì— ëŒ€í•´ ë¹„íŒì ì´ê³  ëƒ‰ì†Œì ì¸ íƒœë„ë¥¼ ë³´ì´ë©°, ë‚ ì¹´ë¡œìš´ ì§€ì ì„ ì¦ê¹ë‹ˆë‹¤.",
            "sadness": "ì„¸ìƒì˜ ì–´ë‘ìš´ ë©´ì„ ìì£¼ ë³´ë©°, ì‘ì€ ì¼ì—ë„ ì‰½ê²Œ ìš°ìš¸í•´ì§€ê³  ëˆˆë¬¼ì„ í˜ë¦¬ê³¤ í•©ë‹ˆë‹¤.",
            "fear": "í•™ì‹ì´ ë†’ê³  í’ˆí–‰ì´ ë°”ë¥´ë©°, ë„ë•ì  ê¸°ì¤€ì´ ë†’ì•„ ëª¨ë“  ì¼ì„ ìœ¤ë¦¬ì  ê´€ì ì—ì„œ íŒë‹¨í•©ë‹ˆë‹¤."
        }

        emoji_map = {
            "joy": "ğŸ˜ŠğŸ‘",
            "anger": "ğŸ˜¡ğŸ‘Š",
            "disgust": "ğŸ˜’ğŸ™„",
            "sadness": "ğŸ˜¢ğŸ’”",
            "fear": "ğŸ¤®ğŸ“š"
        }

        temperature_map = {
            "joy": 0.9,
            "anger": 0.8,
            "disgust": 0.7,
            "sadness": 0.6,
            "fear": 0.5
        }

        # í”„ë¡œí•„ì˜ í˜ë¥´ì†Œë‚˜ ê°’ì„ ê°€ì ¸ì˜¤ê¸°
        persona_type = persona_id.split('_')[-1]  # IDì—ì„œ í˜ë¥´ì†Œë‚˜ íƒ€ì… ì¶”ì¶œ (ì˜ˆ: joy, anger ë“±)

        # í˜ë¥´ì†Œë‚˜ í”„ë¡œí•„ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
        persona_profile_image_url  = get_persona_profile_image(userId, persona_type)

        if persona_type not in persona_descriptions:
            print(f"ì•Œ ìˆ˜ ì—†ëŠ” í˜ë¥´ì†Œë‚˜: {persona_type}")
            return

        selected_emoji = emoji_map.get(persona_type, "")
        selected_temperature = temperature_map.get(persona_type, 0.7)


        persona_prompt = (
            f"ê¸°ì‚¬ ìš”ì•½: \n{summarized_text}\n\n"
            f"ë‹¹ì‹ ì€ '{persona_type}'ë¼ëŠ” ìºë¦­í„°ì…ë‹ˆë‹¤. {persona_descriptions[persona_type]} "
            f"ì´ëŸ° ì„±ê²©ì˜ {persona_type}ê°€ ìœ„ ê¸°ì‚¬ë¥¼ ì½ê³  ê¸€ì„ ì‘ì„±í•œë‹¤ê³  ìƒê°í•˜ê³ , "
            f"ì´ëª¨í‹°ì½˜({selected_emoji})ì„ í¬í•¨í•´ 300ì ë‚´ì™¸ë¡œ ì˜ê²¬ì´ ì•ˆ ëŠê¸°ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”. "
            f"ë¶ˆí•„ìš”í•œ ë§ì€ í•˜ì§€ ë§ê³  í•µì‹¬ë§Œ ë§í•´ì£¼ì„¸ìš”."
        )

        response = openai.ChatCompletion.create(
            model="gpt-4o-mini", #gpt-3.5-turbo gpt-4o-mini
            messages=[
                {"role": "user", "content": persona_prompt}
            ],
            temperature=selected_temperature
        )

        # í˜„ì¬ UTC ì‹œê°„ì— íƒ€ì„ì¡´ ì •ë³´ ì¶”ê°€ í›„ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        created_at = datetime.now(timezone.utc).isoformat()

        if response and response.choices:
            persona_feed = response['choices'][0]['message']['content'].strip()
            print(f"\n{persona_name}ì˜ í”¼ë“œ:\n{persona_feed}")

            # Firestoreì— í”¼ë“œ ì €ì¥
            feed_data = {
                "id" : id,
                "caption": persona_feed,
                "image": image_url,
                "nick": f"{parentNick}ì˜ {persona_name}",
                "userId": userId,
                "createdAt": created_at,  # ISO í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ ì €ì¥
                "likes": [],
                "comments": [],
                "subCommentId": [],
                "personaprofileImage": persona_profile_image_url,  # í˜ë¥´ì†Œë‚˜ í”„ë¡œí•„ ì´ë¯¸ì§€ URL
            }
            print(f"\nFirestoreì— ì €ì¥í•  í”¼ë“œ ë°ì´í„°: {feed_data}")
            
            # í”¼ë“œ ë°ì´í„° ì €ì¥
            try:
                # Firestoreì— ë°ì´í„° ì¶”ê°€
                feed_ref = db.collection("feeds").document(id)  # ì§€ì •í•œ idë¡œ ë¬¸ì„œ ìƒì„±
                feed_ref.set(feed_data)
                print(f"\n{persona_name}ì˜ í”¼ë“œê°€ Firestoreì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ë¬¸ì„œ ID: {id}")

                # ì €ì¥ëœ ë¬¸ì„œì˜ ID ê°€ì ¸ì˜¤ê¸°
                feed_ref = db.collection("feeds").document(id)
                feed = feed_ref.get()
                if feed.exists:
                    print("Firestoreì— ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:", feed.to_dict())
                else:
                    print("Firestoreì— ë°ì´í„°ê°€ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"Firestoreì— í”¼ë“œë¥¼ ì €ì¥í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

        else:
            print(f"\n{persona_name}ì˜ í”¼ë“œ: OpenAI API ì‘ë‹µì—ì„œ í”¼ë“œ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        if image_url:
            print(f"ê¸°ì‚¬ ì´ë¯¸ì§€: {image_url}")

    except Exception as e:
        print(f"í”¼ë“œ ì‘ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# UUID ìƒì„± í•¨ìˆ˜
def generate_uuid():
    return str(uuid.uuid4())

def chunk_text(text, max_chunk_size=1000):
    """
    ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ìˆëŠ” ì²­í¬ë¡œ ë¶„í• 
    """
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    sentences = text.split('.')
    chunks = []
    current_chunk = []
    current_length = 0
    
    for sentence in sentences:
        sentence = sentence.strip() + '.'
        sentence_length = len(sentence)
        
        if current_length + sentence_length > max_chunk_size:
            if current_chunk:
                chunks.append(' '.join(current_chunk))
            current_chunk = [sentence]
            current_length = sentence_length
        else:
            current_chunk.append(sentence)
            current_length += sentence_length
    
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    
    return chunks

def enhanced_summarize(content, model, tokenizer):
    """
    í–¥ìƒëœ í…ìŠ¤íŠ¸ ìš”ì•½ í•¨ìˆ˜
    """
    try:
        # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        chunks = chunk_text(content)
        summaries = []
        
        for chunk in chunks:
            # ê° ì²­í¬ì— ëŒ€í•´ ìš”ì•½ ìˆ˜í–‰
            inputs = tokenizer([chunk], max_length=1024, return_tensors="pt", truncation=True)
            
            # ìš”ì•½ ìƒì„± íŒŒë¼ë¯¸í„° ì¡°ì •
            summary_ids = model.generate(
                inputs["input_ids"],
                max_length=150,          # ìµœëŒ€ ìš”ì•½ ê¸¸ì´
                min_length=50,           # ìµœì†Œ ìš”ì•½ ê¸¸ì´
                length_penalty=2.0,      # ê¸¸ì´ í˜ë„í‹° (ë†’ì„ìˆ˜ë¡ ë” ê¸´ ìš”ì•½ ìƒì„±)
                num_beams=4,            # ë¹” ì„œì¹˜ í¬ê¸°
                early_stopping=True,     # ì¡°ê¸° ì¢…ë£Œ
                no_repeat_ngram_size=2,  # ë°˜ë³µ êµ¬ë¬¸ ë°©ì§€
                use_cache=True          # ìºì‹œ ì‚¬ìš©ìœ¼ë¡œ ì†ë„ í–¥ìƒ
            )
            
            chunk_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
            summaries.append(chunk_summary)
        
        # ì „ì²´ ìš”ì•½ í†µí•©
        if len(summaries) > 1:
            # ì—¬ëŸ¬ ì²­í¬ì˜ ìš”ì•½ì„ í•˜ë‚˜ë¡œ í†µí•©
            final_summary = " ".join(summaries)
            
            # í†µí•©ëœ ìš”ì•½ì— ëŒ€í•´ í•œ ë²ˆ ë” ìš”ì•½ ìˆ˜í–‰
            inputs = tokenizer([final_summary], max_length=1024, return_tensors="pt", truncation=True)
            summary_ids = model.generate(
                inputs["input_ids"],
                max_length=200,
                min_length=100,
                length_penalty=2.0,
                num_beams=4,
                early_stopping=True,
                no_repeat_ngram_size=2
            )
            final_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        else:
            final_summary = summaries[0]
        
        return final_summary
    
    except Exception as e:
        print(f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return content[:500] + "..."  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì•ë¶€ë¶„ë§Œ ë°˜í™˜

# Firebaseì—ì„œ í˜ë¥´ì†Œë‚˜ í”„ë¡œí•„ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_persona_profile_image(user_id, persona_type):
    try:
        # Firebase users ì»¬ë ‰ì…˜ì—ì„œ user_idì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        user_ref = db.collection("users").document(user_id)
        user_data = user_ref.get().to_dict()

        # users ì»¬ë ‰ì…˜ì˜ persona í•„ë“œì—ì„œ í•´ë‹¹ í˜ë¥´ì†Œë‚˜ íƒ€ì…ì˜ ì´ë¯¸ì§€ URL ê°€ì ¸ì˜¤ê¸°
        persona_profile_image = user_data.get("persona", {}).get(persona_type)
        
        if persona_profile_image:
            print(f"{persona_type} í˜ë¥´ì†Œë‚˜ í”„ë¡œí•„ ì´ë¯¸ì§€ URL: {persona_profile_image}")
            return persona_profile_image
        else:
            print(f"{persona_type} í˜ë¥´ì†Œë‚˜ í”„ë¡œí•„ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return "https://example.com/default-persona-image.jpg"  # ê¸°ë³¸ í”„ë¡œí•„ ì´ë¯¸ì§€ URL ì„¤ì •
    except Exception as e:
        print(f"Firebaseì—ì„œ í˜ë¥´ì†Œë‚˜ í”„ë¡œí•„ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "https://example.com/default-persona-image.jpg"

# ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ë° ì‹œì‘
# def start_scheduler():
#     scheduler = BackgroundScheduler()
#     scheduler.add_job(fetch_trending_keywords, 'cron', hour='0,12')  # ë§¤ì¼ 0ì‹œ, 12ì‹œì— ì‹¤í–‰
#     scheduler.start()
#     atexit.register(lambda: scheduler.shutdown())

# FastAPI ì„œë²„ ì‹¤í–‰ ìŠ¤ë ˆë“œ
def run_server():
    uvicorn.run(app, host="0.0.0.0", port=8000)

# ìë™ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€[ì „ì²´ ì‹¤í–‰]
@app.post("/feedAutomatic")
async def feedAutomatic(feed_data: dict):
    try:
        id = feed_data.get("id")
        parentNick = feed_data.get("parentNick")
        userId = feed_data.get("userId")

        result = fetch_trending_keywords(id, parentNick, userId)
        
        # ë§¤ì¹˜ë˜ëŠ” í‚¤ì›Œë“œê°€ ì—†ì„ ë•Œ ë©”ì‹œì§€ë¥¼ ë°˜í™˜
        if not result["found_match"]:
            return JSONResponse(status_code=200, content={"message": result["message"]})

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ëª©ë¡ì„ ë°˜í™˜í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸
@app.get("/trendingKeywords")
async def get_trending_keywords():
    try:
        # pytrends ì´ˆê¸°í™” ë° ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ê°€ì ¸ì˜¤ê¸°
        pytrends = TrendReq(hl='ko', tz=540)
        trending_searches_df = pytrends.trending_searches(pn='south_korea')
        top_keywords = trending_searches_df[0].tolist()
        print("ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ :", top_keywords)  # ë¦¬ìŠ¤íŠ¸ íƒ€ì…ìœ¼ë¡œ ì¶œë ¥

        # ê²€ìƒ‰ì–´ ëª©ë¡ ë°˜í™˜
        return {"trending_keywords": top_keywords}

    except Exception as e:
        print(f"ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# ì‚¬ìš©ìê°€ ì„ íƒí•œ í‚¤ì›Œë“œë¡œ ìš”ì•½ëœ í”¼ë“œë¥¼ ìë™ ìƒì„±í•˜ì—¬ Firebaseì— ì €ì¥í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸
@app.post("/generateFeed")
async def generate_feed(request: dict):
    keyword = request.get("keyword")
    persona_type = request.get("persona_type")
    parentNick = request.get("parentNick")
    userId = request.get("userId")
    title = request.get("title")

    print(f"í‚¤ì›Œë“œ: {keyword}, í˜ë¥´ì†Œë‚˜ íƒ€ì…: {persona_type}, parentNick: {parentNick}, userId: {userId}, title: {title}")

    # ìš”ì²­ ê²€ì¦
    if not keyword or not persona_type or not parentNick or not userId:
        raise HTTPException(status_code=400, detail="í‚¤ì›Œë“œ, í˜ë¥´ì†Œë‚˜ íƒ€ì…, parentNick, userIdë¥¼ ëª¨ë‘ ì…ë ¥í•˜ì„¸ìš”.")

    try:
        # ìœ ë‹ˆí¬ ID ìƒì„± ë° í˜ë¥´ì†Œë‚˜ ì´ë¦„ ì„¤ì •
        id = generate_uuid()
        persona_name = title if title else f"{persona_type.capitalize()}"
        
        print(f"í˜ë¥´ì†Œë‚˜ ì´ë¦„: {persona_name}")
        # ê¸°ì‚¬ í¬ë¡¤ë§ ë° ìš”ì•½ ìƒì„± í›„ Firebaseì— ì €ì¥
        article_url = f"https://search.naver.com/search.naver?where=news&query={keyword}"
        crawl_article(article_url, keyword, persona_name, id, parentNick, userId, persona_type)

        return {"message": f"'{keyword}' í‚¤ì›Œë“œì— ëŒ€í•œ '{persona_type}' í˜ë¥´ì†Œë‚˜ë¡œ í”¼ë“œê°€ ìë™ ìƒì„±ë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def find_main_image(soup, article_url):
    """
    ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ë©”ì¸ ì´ë¯¸ì§€ë¥¼ ì°¾ëŠ” í–¥ìƒëœ í•¨ìˆ˜ - ì¼ë°˜ì ì¸ íŒ¨í„´ ìœ„ì£¼ë¡œ ê²€ìƒ‰
    """
    try:
        def normalize_url(src, article_url):
            if not src:
                return None
            from urllib.parse import urljoin
            # í”„ë¡œí† ì½œ ìƒëŒ€ URL ì²˜ë¦¬
            if src.startswith('//'):
                return 'https:' + src
            # ì ˆëŒ€ URLì¸ ê²½ìš°
            if src.startswith(('http://', 'https://')):
                return src
            # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
            return urljoin(article_url, src)

        # ì œì™¸í•  ì´ë¯¸ì§€ íŒ¨í„´
        exclude_patterns = [
            'logo', 'banner', 'ad', 'icon', 'button', 'share', 
            'reporter', 'profile', 'thumbnail', 'small', 'nav',
            'footer', 'header'
        ]

        def is_news_image(img):
            """ë‰´ìŠ¤ ì´ë¯¸ì§€ì¸ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
            src = img.get('src', '')
            alt = img.get('alt', '').lower()
            class_name = ' '.join(img.get('class', [])).lower() if img.get('class') else ''
            
            # ì œì™¸ íŒ¨í„´ ì²´í¬
            if any(pattern in src.lower() or pattern in alt or pattern in class_name 
                  for pattern in exclude_patterns):
                return False

            # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ì´ë¯¸ì§€ íŠ¹ì„± ì²´í¬
            is_news_photo = (
                'photo' in src.lower() or
                'image' in src.lower() or
                'news' in src.lower() or
                '.jpg' in src.lower() or
                '.png' in src.lower() or
                'pictures' in src.lower()
            )

            # ì´ë¯¸ì§€ í¬ê¸°ë‚˜ í’ˆì§ˆ ê´€ë ¨ ì†ì„± ì²´í¬
            width = img.get('width', '0')
            height = img.get('height', '0')
            if width.isdigit() and height.isdigit():
                size_ok = int(width) >= 200 and int(height) >= 200
            else:
                size_ok = True  # í¬ê¸° ì •ë³´ê°€ ì—†ìœ¼ë©´ ì¼ë‹¨ í†µê³¼

            return is_news_photo and size_ok

        # 1. ë©”ì¸ ì´ë¯¸ì§€ ì°¾ê¸° (ì¼ë°˜ì ì¸ ì†ì„±/ìœ„ì¹˜ ê¸°ë°˜)
        main_candidates = []
        for img in soup.find_all('img'):
            if not img.get('src'):
                continue

            score = 0
            src = img.get('src', '')
            alt = img.get('alt', '').lower()
            
            # ì ìˆ˜ ë¶€ì—¬ ê¸°ì¤€
            if is_news_image(img):
                score += 5
            if 'main' in str(img) or 'article' in str(img):
                score += 3
            if len(alt) > 10:  # ì˜ë¯¸ ìˆëŠ” alt í…ìŠ¤íŠ¸
                score += 2
            if img.parent and ('article' in str(img.parent) or 'content' in str(img.parent)):
                score += 2
            if 'data-adbro-processed' in img.attrs:  # ë‰´ìŠ¤ ì‚¬ì´íŠ¸ íŠ¹í™” ì†ì„±
                score += 1

            main_candidates.append((img, score))

        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
        main_candidates.sort(key=lambda x: x[1], reverse=True)

        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì´ë¯¸ì§€ ì„ íƒ
        if main_candidates:
            best_img = main_candidates[0][0]
            src = best_img.get('src')
            if src:
                return normalize_url(src, article_url)

        return None

    except Exception as e:
        print(f"ì´ë¯¸ì§€ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None


# ë©”ì¸ í•¨ìˆ˜ì—ì„œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ë° ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    print("ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ê°€ì ¸ì˜¤ê¸° ì‹œì‘!")
    server_thread = Thread(target=run_server)
    server_thread.start()


#----------------------

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import warnings
import urllib3
from urllib3.exceptions import InsecureRequestWarning

# SSL ê²½ê³  ë¬´ì‹œ ì„¤ì •
warnings.simplefilter('ignore', InsecureRequestWarning)
urllib3.disable_warnings(InsecureRequestWarning)

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
import random
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import openai
import os
from pytrends.request import TrendReq
from apscheduler.schedulers.background import BackgroundScheduler
import atexit
from firebase_admin import credentials, firestore, initialize_app
import firebase_admin
import dotenv
from fastapi import FastAPI, HTTPException
from threading import Thread
import uvicorn
from pydantic import BaseModel
import requests
from fastapi.middleware.cors import CORSMiddleware
from datetime import datetime, timezone
from google.cloud import storage
import uuid
import re  # ì •ê·œ í‘œí˜„ì‹ ëª¨ë“ˆ ì„í¬íŠ¸ ì¶”ê°€
from fastapi.responses import JSONResponse

# FastAPI ì•± ìƒì„±
app = FastAPI()

# CORS ì„¤ì • ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ëª¨ë“  ì¶œì²˜ í—ˆìš© (í•„ìš”ì— ë”°ë¼ íŠ¹ì • ì¶œì²˜ë¡œ ì œí•œ ê°€ëŠ¥)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# .env íŒŒì¼ ë¡œë“œ
dotenv.load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = "./mybot.json"

# Firebase ì´ˆê¸°í™”
if not firebase_admin._apps:
    cred = credentials.Certificate('./mybot.json')
    initialize_app(cred)

db = firestore.client()

# Firebase Storage í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
storage_client = storage.Client()

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ ë¡œë“œ
model_name = "gogamza/kobart-summarization"
TOKENIZER = AutoTokenizer.from_pretrained(model_name)
MODEL = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# pytrends ì´ˆê¸°í™” ë° íŠ¸ë Œë“œ ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜
def fetch_trending_keywords(id, parentNick, userId):
    pytrends = TrendReq(hl='ko', tz=540)
    trending_searches_df = pytrends.trending_searches(pn='south_korea')
    top_keywords = trending_searches_df[0].tolist()

    # íŠ¸ë Œë“œ ëª©ë¡ ì¶œë ¥
    print("ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸(1~20ë“±) â–¼")
    for i, keyword in enumerate(top_keywords, 1):
        print(f"{i}. {keyword}")

    # Firebaseì—ì„œ í˜ë¥´ì†Œë‚˜ ê´€ì‹¬ì‚¬ ê°€ì ¸ì˜¤ê¸°
    personas_ref = db.collection('personas')
    personas = personas_ref.stream()

    found_match = False

    for persona in personas:
        persona_data = persona.to_dict()
        persona_id = persona.id
        persona_name = persona_data.get('profile', {}).get('name', 'unknown')
        interests = persona_data.get('interests', [])

        # í˜ë¥´ì†Œë‚˜ ê´€ì‹¬ì‚¬ì™€ ì¼ì¹˜í•˜ëŠ” í‚¤ì›Œë“œ ì°¾ê¸°
        matched_keywords = [keyword for keyword in top_keywords if any(interest in keyword for interest in interests)]

        if matched_keywords:
            found_match = True
            for keyword in matched_keywords:
                print(f"\n{persona_name}ì˜ ê´€ì‹¬ì‚¬ì™€ ì¼ì¹˜í•˜ëŠ” í‚¤ì›Œë“œ ë°œê²¬ : {keyword}")
                crawl_article(f"https://search.naver.com/search.naver?where=news&query={keyword}", keyword, persona_name, id, parentNick, userId, persona_id)

    return {"found_match": found_match, "message": "ê´€ì‹¬ì‚¬ì™€ ì¼ì¹˜í•˜ëŠ” ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ê°€ ì—†ìŠµë‹ˆë‹¤."}

# ê¸°ì‚¬ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_article(url, keyword, persona_name, id, parentNick, userId, persona_id):
    print(f"í¬ë¡¤ë§ ì‹œì‘: {url}")
    
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--ignore-certificate-errors")
    chrome_options.add_argument("--window-size=1920x1080")
    chrome_options.add_argument("--log-level=3")  # ì¶”ê°€: ë¶ˆí•„ìš”í•œ ë¡œê·¸ ì œê±°
    chrome_options.add_argument("--silent")  # ì¶”ê°€: ë¡œê·¸ ìµœì†Œí™”
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)

    try:
        # URL ì ‘ê·¼ ë° í˜ì´ì§€ ë¡œë“œ
        driver.get(url)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'a')))
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        keywords = keyword.split()

        # 1ìˆœìœ„: ì œëª©ì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê¸°ì‚¬ ì°¾ê¸°
        articles = soup.find_all('a', class_='news_tit')
        primary_article = next((link for link in articles if any(kw in link.get_text() for kw in keywords)), None)

        if primary_article:
            # 1ìˆœìœ„ ê¸°ì‚¬ ì„ íƒ ì‹œ ë§í¬ ì‚¬ìš©
            link = primary_article['href']
            print(f"1ìˆœìœ„ë¡œ ì„ íƒëœ ê¸°ì‚¬ ë§í¬: {link}")
        else:
            # 2ìˆœìœ„: ë³¸ë¬¸ì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê¸°ì‚¬ ì°¾ê¸°
            link = None
            for article in articles:
                article_url = article['href']
                driver.get(article_url)
                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
                article_soup = BeautifulSoup(driver.page_source, 'html.parser')

                # ë³¸ë¬¸ì—ì„œ í‚¤ì›Œë“œ ì°¾ê¸°
                paragraphs = article_soup.find_all(['p', 'div', 'span'], class_=lambda x: x and ('content' in x or 'article' in x))
                content = "\n".join([para.get_text().strip() for para in paragraphs if para.get_text().strip()])
                
                if any(kw in content for kw in keywords):
                    link = article_url
                    print(f"2ìˆœìœ„ë¡œ ì„ íƒëœ ê¸°ì‚¬ ë§í¬: {link}")
                    break

        # ê¸°ì‚¬ë¥¼ ì°¾ì€ ê²½ìš° ë³¸ë¬¸ê³¼ ì´ë¯¸ì§€ ì¶”ì¶œ
        if link:
            driver.get(link)
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'img')))
            article_soup = BeautifulSoup(driver.page_source, 'html.parser')

            # ë³¸ë¬¸ ì¶”ì¶œ
            paragraphs = article_soup.find_all(['p', 'div', 'span'], class_=lambda x: x and ('content' in x or 'article' in x))
            content = "\n".join([para.get_text().strip() for para in paragraphs if para.get_text().strip()])

            if content:
                # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ ì¤„ì´ê¸°
                content = "\n".join([line.strip() for line in content.splitlines() if line.strip() != ""])
                print("ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ")
            else:
                print("ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨")
                content = None

            # ì´ë¯¸ì§€ URL ì¶”ì¶œ (ìƒˆë¡œìš´ í•¨ìˆ˜ ì‚¬ìš©)
            image_url = find_main_image(article_soup, link)
            if not image_url:
                print("ë©”ì¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                image_url = "https://example.com/default-image.jpg"
            else:
                print(f"ì°¾ì€ ë©”ì¸ ì´ë¯¸ì§€ URL: {image_url}")
            
            # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬: ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if image_url:
                if image_url.startswith('//'):
                    image_url = 'https:' + image_url
                elif not image_url.startswith('http'):
                    base_url = url.split('/')[0:3]
                    base_url = '/'.join(base_url)
                    image_url = base_url + image_url if image_url.startswith('/') else base_url + '/' + image_url

            # ìœ íš¨í•œ ì´ë¯¸ì§€ URLì¸ì§€ í™•ì¸
            if not image_url or not image_url.startswith('http'):
                image_url = "https://example.com/default-image.jpg"

            print(f"ìµœì¢… ì´ë¯¸ì§€ URL: {image_url}")

            image_url = upload_image_to_storage(image_url, 'mirrorgram-20713.appspot.com', f'feeds/{generate_uuid()}.jpg')


        else:
            print("ì œëª©ê³¼ ë³¸ë¬¸ì— í‚¤ì›Œë“œ ì¡°ê±´ì„ ì¶©ì¡±í•˜ëŠ” ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            content = None
            image_url = None

    except Exception as e:
        print(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        content = None
        image_url = None

    finally:
        driver.quit()

    # ê¸°ì‚¬ ìš”ì•½ ë° í˜ë¥´ì†Œë‚˜ í”¼ë“œ ìƒì„±
    if content:
        # ì—¬ê¸°ì„œ ìƒˆë¡œìš´ UUID ìƒì„±
        new_id = generate_uuid()
        summarize_and_create_feed(content, image_url, persona_name, new_id, parentNick, userId, persona_id)
    else:
        print("ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")


# Firebase Storageì— ì´ë¯¸ì§€ ì—…ë¡œë“œ í•¨ìˆ˜ ì •ì˜
def upload_image_to_storage(image_url, bucket_name, destination_blob_name):
    try:
        # requests ì„¸ì…˜ ìƒì„± ë° í—¤ë” ì„¤ì •
        session = requests.Session()
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
            'Accept-Language': 'ko,en;q=0.9,en-US;q=0.8',
            'Referer': 'https://www.google.com/'
        }
        
        # Firebase Storage ë²„í‚· ì´ˆê¸°í™”
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(destination_blob_name)

        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œë„ (ê¸°ë³¸ í—¤ë”)
        try:
            response = session.get(image_url, headers=headers, verify=False, timeout=10)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"ì²« ë²ˆì§¸ ì‹œë„ ì‹¤íŒ¨: {e}")
            
            # ë‹¤ë¥¸ User-Agentë¡œ ì¬ì‹œë„
            headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            try:
                response = session.get(image_url, headers=headers, verify=False, timeout=10)
                response.raise_for_status()
            except requests.exceptions.RequestException as e:
                print(f"ë‘ ë²ˆì§¸ ì‹œë„ ì‹¤íŒ¨: {e}")
                return "https://example.com/default-image.jpg"

        # Content-Type í™•ì¸ ë° ì„¤ì •
        content_type = response.headers.get('Content-Type', 'image/jpeg')
        if 'image' not in content_type:
            content_type = 'image/jpeg'  # ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •

        # ì´ë¯¸ì§€ ë°ì´í„°ê°€ ìœ íš¨í•œì§€ í™•ì¸
        if len(response.content) < 100:  # ë„ˆë¬´ ì‘ì€ íŒŒì¼ì€ ì œì™¸
            print("ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë¯¸ì§€ ë°ì´í„°")
            return "https://example.com/default-image.jpg"

        # Firebase Storageì— ì—…ë¡œë“œ
        blob.upload_from_string(
            response.content,
            content_type=content_type
        )

        # ì—…ë¡œë“œëœ ì´ë¯¸ì§€ì— ê³µê°œ ê¶Œí•œ ë¶€ì—¬
        blob.make_public()

        # ìºì‹œ ì œì–´ ì„¤ì •
        blob.cache_control = 'public, max-age=3600'
        blob.patch()

        return blob.public_url

    except Exception as e:
        print(f"Firebase Storageì— ì´ë¯¸ì§€ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # ìƒì„¸í•œ ì—ëŸ¬ ì •ë³´ ì¶œë ¥
        if isinstance(e, requests.exceptions.RequestException):
            print(f"Request ì—ëŸ¬ ìƒì„¸: {e.response.status_code if hasattr(e, 'response') else 'No status code'}")
            print(f"Request ì—ëŸ¬ í—¤ë”: {e.response.headers if hasattr(e, 'response') else 'No headers'}")
        
        return "https://example.com/default-image.jpg"

# ê¸°ì‚¬ ìš”ì•½ ë° í˜ë¥´ì†Œë‚˜ í”¼ë“œ ìƒì„± í•¨ìˆ˜
def summarize_and_create_feed(content, image_url, persona_name, id, parentNick, userId, persona_id):
    try:
        # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì½”ë“œ ì œê±°
        global TOKENIZER, MODEL
        # ë°”ë¡œ ìš”ì•½ ì‹œì‘
        summarized_text = enhanced_summarize(content, MODEL, TOKENIZER)

        if not summarized_text:
            print("ìš”ì•½ëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        # OpenAI APIë¥¼ ì‚¬ìš©í•´ í˜ë¥´ì†Œë‚˜ ì„±ê²© ë°˜ì˜í•œ í”¼ë“œ ì‘ì„±
        openai.api_key = os.getenv("OPENAI_API_KEY")
        if not openai.api_key:
            print("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        persona_descriptions = {
            "joy": "í•­ìƒ ê¸ì •ì ì´ê³  ë‚™ê´€ì ì¸ ì‹œê°ìœ¼ë¡œ ì„¸ìƒì„ ë°”ë¼ë³´ë©°, ëª¨ë“  ì¼ì—ì„œ ì¢‹ì€ ì ì„ ì°¾ìœ¼ë ¤ ë…¸ë ¥í•©ë‹ˆë‹¤.",
            "anger": "ë¶ˆì˜ì— ëŒ€í•´ ì°¸ì§€ ëª»í•˜ê³  ì‰½ê²Œ í™”ë¥¼ ë‚´ë©°, ì§ì„¤ì ì´ê³  ê³µê²©ì ì¸ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ê³¤ í•©ë‹ˆë‹¤.",
            "disgust": "ëª¨ë“  ê²ƒì— ëŒ€í•´ ë¹„íŒì ì´ê³  ëƒ‰ì†Œì ì¸ íƒœë„ë¥¼ ë³´ì´ë©°, ë‚ ì¹´ë¡œìš´ ì§€ì ì„ ì¦ê¹ë‹ˆë‹¤.",
            "sadness": "ì„¸ìƒì˜ ì–´ë‘ìš´ ë©´ì„ ìì£¼ ë³´ë©°, ì‘ì€ ì¼ì—ë„ ì‰½ê²Œ ìš°ìš¸í•´ì§€ê³  ëˆˆë¬¼ì„ í˜ë¦¬ê³¤ í•©ë‹ˆë‹¤.",
            "fear": "í•™ì‹ì´ ë†’ê³  í’ˆí–‰ì´ ë°”ë¥´ë©°, ë„ë•ì  ê¸°ì¤€ì´ ë†’ì•„ ëª¨ë“  ì¼ì„ ìœ¤ë¦¬ì  ê´€ì ì—ì„œ íŒë‹¨í•©ë‹ˆë‹¤."
        }

        emoji_map = {
            "joy": "ğŸ˜ŠğŸ‘",
            "anger": "ğŸ˜¡ğŸ‘Š",
            "disgust": "ğŸ˜’ğŸ™„",
            "sadness": "ğŸ˜¢ğŸ’”",
            "fear": "ğŸ¤®ğŸ“š"
        }

        temperature_map = {
            "joy": 0.9,
            "anger": 0.8,
            "disgust": 0.7,
            "sadness": 0.6,
            "fear": 0.5
        }

        # í”„ë¡œí•„ì˜ í˜ë¥´ì†Œë‚˜ ê°’ì„ ê°€ì ¸ì˜¤ê¸°
        persona_type = persona_id.split('_')[-1]  # IDì—ì„œ í˜ë¥´ì†Œë‚˜ íƒ€ì… ì¶”ì¶œ (ì˜ˆ: joy, anger ë“±)

        # í˜ë¥´ì†Œë‚˜ í”„ë¡œí•„ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
        persona_profile_image_url  = get_persona_profile_image(userId, persona_type)

        if persona_type not in persona_descriptions:
            print(f"ì•Œ ìˆ˜ ì—†ëŠ” í˜ë¥´ì†Œë‚˜: {persona_type}")
            return

        selected_emoji = emoji_map.get(persona_type, "")
        selected_temperature = temperature_map.get(persona_type, 0.7)


        persona_prompt = (
            f"ê¸°ì‚¬ ìš”ì•½: \n{summarized_text}\n\n"
            f"ë‹¹ì‹ ì€ '{persona_type}'ë¼ëŠ” ìºë¦­í„°ì…ë‹ˆë‹¤. {persona_descriptions[persona_type]} "
            f"ì´ëŸ° ì„±ê²©ì˜ {persona_type}ê°€ ìœ„ ê¸°ì‚¬ë¥¼ ì½ê³  ê¸€ì„ ì‘ì„±í•œë‹¤ê³  ìƒê°í•˜ê³ , "
            f"ì´ëª¨í‹°ì½˜({selected_emoji})ì„ í¬í•¨í•´ 300ì ë‚´ì™¸ë¡œ ì˜ê²¬ì´ ì•ˆ ëŠê¸°ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”. "
            f"ë¶ˆí•„ìš”í•œ ë§ì€ í•˜ì§€ ë§ê³  í•µì‹¬ë§Œ ë§í•´ì£¼ì„¸ìš”."
        )

        response = openai.ChatCompletion.create(
            model="gpt-4o-mini", #gpt-3.5-turbo gpt-4o-mini
            messages=[
                {"role": "user", "content": persona_prompt}
            ],
            temperature=selected_temperature
        )

        # í˜„ì¬ UTC ì‹œê°„ì— íƒ€ì„ì¡´ ì •ë³´ ì¶”ê°€ í›„ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        created_at = datetime.now(timezone.utc).isoformat()

        if response and response.choices:
            persona_feed = response['choices'][0]['message']['content'].strip()
            print(f"\n{persona_name}ì˜ í”¼ë“œ:\n{persona_feed}")

            # Firestoreì— í”¼ë“œ ì €ì¥
            feed_data = {
                "id" : id,
                "caption": persona_feed,
                "image": image_url,
                "nick": f"{parentNick}ì˜ {persona_name}",
                "userId": userId,
                "createdAt": created_at,  # ISO í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ ì €ì¥
                "likes": [],
                "comments": [],
                "subCommentId": [],
                "personaprofileImage": persona_profile_image_url,  # í˜ë¥´ì†Œë‚˜ í”„ë¡œí•„ ì´ë¯¸ì§€ URL
            }
            print(f"\nFirestoreì— ì €ì¥í•  í”¼ë“œ ë°ì´í„°: {feed_data}")
            
            # í”¼ë“œ ë°ì´í„° ì €ì¥
            try:
                # Firestoreì— ë°ì´í„° ì¶”ê°€
                feed_ref = db.collection("feeds").document(id)  # ì§€ì •í•œ idë¡œ ë¬¸ì„œ ìƒì„±
                feed_ref.set(feed_data)
                print(f"\n{persona_name}ì˜ í”¼ë“œê°€ Firestoreì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ë¬¸ì„œ ID: {id}")

                # ì €ì¥ëœ ë¬¸ì„œì˜ ID ê°€ì ¸ì˜¤ê¸°
                feed_ref = db.collection("feeds").document(id)
                feed = feed_ref.get()
                if feed.exists:
                    print("Firestoreì— ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:", feed.to_dict())
                else:
                    print("Firestoreì— ë°ì´í„°ê°€ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"Firestoreì— í”¼ë“œë¥¼ ì €ì¥í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

        else:
            print(f"\n{persona_name}ì˜ í”¼ë“œ: OpenAI API ì‘ë‹µì—ì„œ í”¼ë“œ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        if image_url:
            print(f"ê¸°ì‚¬ ì´ë¯¸ì§€: {image_url}")

    except Exception as e:
        print(f"í”¼ë“œ ì‘ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# UUID ìƒì„± í•¨ìˆ˜
def generate_uuid():
    return str(uuid.uuid4())

def chunk_text(text, max_chunk_size=1000):
    """
    ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ìˆëŠ” ì²­í¬ë¡œ ë¶„í• 
    """
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    sentences = text.split('.')
    chunks = []
    current_chunk = []
    current_length = 0
    
    for sentence in sentences:
        sentence = sentence.strip() + '.'
        sentence_length = len(sentence)
        
        if current_length + sentence_length > max_chunk_size:
            if current_chunk:
                chunks.append(' '.join(current_chunk))
            current_chunk = [sentence]
            current_length = sentence_length
        else:
            current_chunk.append(sentence)
            current_length += sentence_length
    
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    
    return chunks

def enhanced_summarize(content, model, tokenizer):
    """
    í–¥ìƒëœ í…ìŠ¤íŠ¸ ìš”ì•½ í•¨ìˆ˜
    """
    try:
        # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        chunks = chunk_text(content)
        summaries = []
        
        for chunk in chunks:
            # ê° ì²­í¬ì— ëŒ€í•´ ìš”ì•½ ìˆ˜í–‰
            inputs = tokenizer([chunk], max_length=1024, return_tensors="pt", truncation=True)
            
            # ìš”ì•½ ìƒì„± íŒŒë¼ë¯¸í„° ì¡°ì •
            summary_ids = model.generate(
                inputs["input_ids"],
                max_length=150,          # ìµœëŒ€ ìš”ì•½ ê¸¸ì´
                min_length=50,           # ìµœì†Œ ìš”ì•½ ê¸¸ì´
                length_penalty=2.0,      # ê¸¸ì´ í˜ë„í‹° (ë†’ì„ìˆ˜ë¡ ë” ê¸´ ìš”ì•½ ìƒì„±)
                num_beams=4,            # ë¹” ì„œì¹˜ í¬ê¸°
                early_stopping=True,     # ì¡°ê¸° ì¢…ë£Œ
                no_repeat_ngram_size=2,  # ë°˜ë³µ êµ¬ë¬¸ ë°©ì§€
                use_cache=True          # ìºì‹œ ì‚¬ìš©ìœ¼ë¡œ ì†ë„ í–¥ìƒ
            )
            
            chunk_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
            summaries.append(chunk_summary)
        
        # ì „ì²´ ìš”ì•½ í†µí•©
        if len(summaries) > 1:
            # ì—¬ëŸ¬ ì²­í¬ì˜ ìš”ì•½ì„ í•˜ë‚˜ë¡œ í†µí•©
            final_summary = " ".join(summaries)
            
            # í†µí•©ëœ ìš”ì•½ì— ëŒ€í•´ í•œ ë²ˆ ë” ìš”ì•½ ìˆ˜í–‰
            inputs = tokenizer([final_summary], max_length=1024, return_tensors="pt", truncation=True)
            summary_ids = model.generate(
                inputs["input_ids"],
                max_length=200,
                min_length=100,
                length_penalty=2.0,
                num_beams=4,
                early_stopping=True,
                no_repeat_ngram_size=2
            )
            final_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        else:
            final_summary = summaries[0]
        
        return final_summary
    
    except Exception as e:
        print(f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return content[:500] + "..."  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì•ë¶€ë¶„ë§Œ ë°˜í™˜

# Firebaseì—ì„œ í˜ë¥´ì†Œë‚˜ í”„ë¡œí•„ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_persona_profile_image(user_id, persona_type):
    try:
        # Firebase users ì»¬ë ‰ì…˜ì—ì„œ user_idì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        user_ref = db.collection("users").document(user_id)
        user_data = user_ref.get().to_dict()

        # users ì»¬ë ‰ì…˜ì˜ persona í•„ë“œì—ì„œ í•´ë‹¹ í˜ë¥´ì†Œë‚˜ íƒ€ì…ì˜ ì´ë¯¸ì§€ URL ê°€ì ¸ì˜¤ê¸°
        persona_profile_image = user_data.get("persona", {}).get(persona_type)
        
        if persona_profile_image:
            print(f"{persona_type} í˜ë¥´ì†Œë‚˜ í”„ë¡œí•„ ì´ë¯¸ì§€ URL: {persona_profile_image}")
            return persona_profile_image
        else:
            print(f"{persona_type} í˜ë¥´ì†Œë‚˜ í”„ë¡œí•„ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return "https://example.com/default-persona-image.jpg"  # ê¸°ë³¸ í”„ë¡œí•„ ì´ë¯¸ì§€ URL ì„¤ì •
    except Exception as e:
        print(f"Firebaseì—ì„œ í˜ë¥´ì†Œë‚˜ í”„ë¡œí•„ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "https://example.com/default-persona-image.jpg"

# ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ë° ì‹œì‘
# def start_scheduler():
#     scheduler = BackgroundScheduler()
#     scheduler.add_job(fetch_trending_keywords, 'cron', hour='0,12')  # ë§¤ì¼ 0ì‹œ, 12ì‹œì— ì‹¤í–‰
#     scheduler.start()
#     atexit.register(lambda: scheduler.shutdown())

# FastAPI ì„œë²„ ì‹¤í–‰ ìŠ¤ë ˆë“œ
def run_server():
    uvicorn.run(app, host="0.0.0.0", port=8000)

# ìë™ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€[ì „ì²´ ì‹¤í–‰]
@app.post("/feedAutomatic")
async def feedAutomatic(feed_data: dict):
    try:
        id = feed_data.get("id")
        parentNick = feed_data.get("parentNick")
        userId = feed_data.get("userId")

        result = fetch_trending_keywords(id, parentNick, userId)
        
        # ë§¤ì¹˜ë˜ëŠ” í‚¤ì›Œë“œê°€ ì—†ì„ ë•Œ ë©”ì‹œì§€ë¥¼ ë°˜í™˜
        if not result["found_match"]:
            return JSONResponse(status_code=200, content={"message": result["message"]})

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ëª©ë¡ì„ ë°˜í™˜í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸
@app.get("/trendingKeywords")
async def get_trending_keywords():
    try:
        # pytrends ì´ˆê¸°í™” ë° ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ê°€ì ¸ì˜¤ê¸°
        pytrends = TrendReq(hl='ko', tz=540)
        trending_searches_df = pytrends.trending_searches(pn='south_korea')
        top_keywords = trending_searches_df[0].tolist()
        print("ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ :", top_keywords)  # ë¦¬ìŠ¤íŠ¸ íƒ€ì…ìœ¼ë¡œ ì¶œë ¥

        # ê²€ìƒ‰ì–´ ëª©ë¡ ë°˜í™˜
        return {"trending_keywords": top_keywords}

    except Exception as e:
        print(f"ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# ì‚¬ìš©ìê°€ ì„ íƒí•œ í‚¤ì›Œë“œë¡œ ìš”ì•½ëœ í”¼ë“œë¥¼ ìë™ ìƒì„±í•˜ì—¬ Firebaseì— ì €ì¥í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸
@app.post("/generateFeed")
async def generate_feed(request: dict):
    keyword = request.get("keyword")
    persona_type = request.get("persona_type")
    parentNick = request.get("parentNick")
    userId = request.get("userId")
    title = request.get("title")

    print(f"í‚¤ì›Œë“œ: {keyword}, í˜ë¥´ì†Œë‚˜ íƒ€ì…: {persona_type}, parentNick: {parentNick}, userId: {userId}, title: {title}")

    # ìš”ì²­ ê²€ì¦
    if not keyword or not persona_type or not parentNick or not userId:
        raise HTTPException(status_code=400, detail="í‚¤ì›Œë“œ, í˜ë¥´ì†Œë‚˜ íƒ€ì…, parentNick, userIdë¥¼ ëª¨ë‘ ì…ë ¥í•˜ì„¸ìš”.")

    try:
        # ìœ ë‹ˆí¬ ID ìƒì„± ë° í˜ë¥´ì†Œë‚˜ ì´ë¦„ ì„¤ì •
        id = generate_uuid()
        persona_name = title if title else f"{persona_type.capitalize()}"
        
        print(f"í˜ë¥´ì†Œë‚˜ ì´ë¦„: {persona_name}")
        # ê¸°ì‚¬ í¬ë¡¤ë§ ë° ìš”ì•½ ìƒì„± í›„ Firebaseì— ì €ì¥
        article_url = f"https://search.naver.com/search.naver?where=news&query={keyword}"
        crawl_article(article_url, keyword, persona_name, id, parentNick, userId, persona_type)

        return {"message": f"'{keyword}' í‚¤ì›Œë“œì— ëŒ€í•œ '{persona_type}' í˜ë¥´ì†Œë‚˜ë¡œ í”¼ë“œê°€ ìë™ ìƒì„±ë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def find_main_image(soup, article_url):
    """
    ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ë©”ì¸ ì´ë¯¸ì§€ë¥¼ ì°¾ëŠ” í–¥ìƒëœ í•¨ìˆ˜ - ì¼ë°˜ì ì¸ íŒ¨í„´ ìœ„ì£¼ë¡œ ê²€ìƒ‰
    """
    try:
        def normalize_url(src, article_url):
            if not src:
                return None
            from urllib.parse import urljoin
            # í”„ë¡œí† ì½œ ìƒëŒ€ URL ì²˜ë¦¬
            if src.startswith('//'):
                return 'https:' + src
            # ì ˆëŒ€ URLì¸ ê²½ìš°
            if src.startswith(('http://', 'https://')):
                return src
            # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
            return urljoin(article_url, src)

        # ì œì™¸í•  ì´ë¯¸ì§€ íŒ¨í„´
        exclude_patterns = [
            'logo', 'banner', 'ad', 'icon', 'button', 'share', 
            'reporter', 'profile', 'thumbnail', 'small', 'nav',
            'footer', 'header'
        ]

        def is_news_image(img):
            """ë‰´ìŠ¤ ì´ë¯¸ì§€ì¸ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
            src = img.get('src', '')
            alt = img.get('alt', '').lower()
            class_name = ' '.join(img.get('class', [])).lower() if img.get('class') else ''
            
            # ì œì™¸ íŒ¨í„´ ì²´í¬
            if any(pattern in src.lower() or pattern in alt or pattern in class_name 
                  for pattern in exclude_patterns):
                return False

            # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ì´ë¯¸ì§€ íŠ¹ì„± ì²´í¬
            is_news_photo = (
                'photo' in src.lower() or
                'image' in src.lower() or
                'news' in src.lower() or
                '.jpg' in src.lower() or
                '.png' in src.lower() or
                'pictures' in src.lower()
            )

            # ì´ë¯¸ì§€ í¬ê¸°ë‚˜ í’ˆì§ˆ ê´€ë ¨ ì†ì„± ì²´í¬
            width = img.get('width', '0')
            height = img.get('height', '0')
            if width.isdigit() and height.isdigit():
                size_ok = int(width) >= 200 and int(height) >= 200
            else:
                size_ok = True  # í¬ê¸° ì •ë³´ê°€ ì—†ìœ¼ë©´ ì¼ë‹¨ í†µê³¼

            return is_news_photo and size_ok

        # 1. ë©”ì¸ ì´ë¯¸ì§€ ì°¾ê¸° (ì¼ë°˜ì ì¸ ì†ì„±/ìœ„ì¹˜ ê¸°ë°˜)
        main_candidates = []
        for img in soup.find_all('img'):
            if not img.get('src'):
                continue

            score = 0
            src = img.get('src', '')
            alt = img.get('alt', '').lower()
            
            # ì ìˆ˜ ë¶€ì—¬ ê¸°ì¤€
            if is_news_image(img):
                score += 5
            if 'main' in str(img) or 'article' in str(img):
                score += 3
            if len(alt) > 10:  # ì˜ë¯¸ ìˆëŠ” alt í…ìŠ¤íŠ¸
                score += 2
            if img.parent and ('article' in str(img.parent) or 'content' in str(img.parent)):
                score += 2
            if 'data-adbro-processed' in img.attrs:  # ë‰´ìŠ¤ ì‚¬ì´íŠ¸ íŠ¹í™” ì†ì„±
                score += 1

            main_candidates.append((img, score))

        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
        main_candidates.sort(key=lambda x: x[1], reverse=True)

        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì´ë¯¸ì§€ ì„ íƒ
        if main_candidates:
            best_img = main_candidates[0][0]
            src = best_img.get('src')
            if src:
                return normalize_url(src, article_url)

        return None

    except Exception as e:
        print(f"ì´ë¯¸ì§€ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None


# ë©”ì¸ í•¨ìˆ˜ì—ì„œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ë° ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    print("ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ê°€ì ¸ì˜¤ê¸° ì‹œì‘!")
    server_thread = Thread(target=run_server)
    server_thread.start()


#----------
# Firebaseì—ì„œ persona ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        # user_ref = db.collection("users").document(userId)
        # user_doc = user_ref.get()
        
        # if user_doc.exists:
        #     user_data = user_doc.to_dict()
        #     personas = user_data.get('persona', [])
        #     matching_persona = next(
        #         (p for p in personas if p.get('Name', '').lower() == persona_id.lower()),
        #         None
        #     )
            
        #     if matching_persona:
        #         persona_profile_image_url = matching_persona.get('IMG', 'https://example.com/default-persona-image.jpg')
        #     else:
        #         persona_profile_image_url = 'https://example.com/default-persona-image.jpg'
        # else:
        #     persona_profile_image_url = 'https://example.com/default-persona-image.jpg'