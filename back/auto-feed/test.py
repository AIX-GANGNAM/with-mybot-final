from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
import random
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import openai
import os
from pytrends.request import TrendReq
from apscheduler.schedulers.background import BackgroundScheduler
import atexit
from firebase_admin import credentials, firestore, initialize_app
import firebase_admin
import dotenv

# .env íŒŒì¼ ë¡œë“œ
dotenv.load_dotenv()

# Firebase ì´ˆê¸°í™”
if not firebase_admin._apps:
    cred = credentials.Certificate('./mybot.json')
    initialize_app(cred)

db = firestore.client()

# pytrends ì´ˆê¸°í™” ë° íŠ¸ë Œë“œ ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜
def fetch_trending_keywords():
    pytrends = TrendReq(hl='ko', tz=540)
    trending_searches_df = pytrends.trending_searches(pn='south_korea')
    top_keywords = trending_searches_df[0].tolist()

    # íŠ¸ë Œë“œ ëª©ë¡ ì¶œë ¥
    print("ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸(1~20ë“±) â–¼")
    for i, keyword in enumerate(top_keywords, 1):
        print(f"{i}. {keyword}")

    # Firebaseì—ì„œ í˜ë¥´ì†Œë‚˜ ê´€ì‹¬ì‚¬ ê°€ì ¸ì˜¤ê¸°
    personas_ref = db.collection('personas')
    personas = personas_ref.stream()

    found_match = False
    for persona in personas:
        persona_data = persona.to_dict()
        persona_id = persona.id  # ë¬¸ì„œ ID ê°€ì ¸ì˜¤ê¸°
        persona_name = persona_id.split('_')[-1]  # ì–¸ë”ë°” ë’¤ì˜ ì´ë¦„ ì¶”ì¶œ
        interests = persona_data.get('interests', [])

        # í˜ë¥´ì†Œë‚˜ ê´€ì‹¬ì‚¬ì™€ ì¼ì¹˜í•˜ëŠ” í‚¤ì›Œë“œ ì°¾ê¸°
        matched_keywords = [keyword for keyword in top_keywords if any(interest in keyword for interest in interests)]
        if matched_keywords:
            found_match = True
            for keyword in matched_keywords:
                print(f"\n{persona_name}ì˜ ê´€ì‹¬ì‚¬ì™€ ì¼ì¹˜í•˜ëŠ” í‚¤ì›Œë“œ ë°œê²¬ : {keyword}")
                crawl_article(f"https://search.naver.com/search.naver?where=news&query={keyword}", keyword, persona_name)

    if not found_match:
        print("ê´€ì‹¬ì‚¬ì™€ ì¼ì¹˜í•˜ëŠ” ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ê¸°ì‚¬ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_article(url, keyword, persona_name):
    # Chrome ì˜µì…˜ ì„¤ì •
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)

    # Chrome WebDriver ì„¤ì •
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)

    try:
        # URL ì ‘ê·¼ ë° í˜ì´ì§€ ë¡œë“œ
        driver.get(url)
        WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, "news_tit")))

        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')
        keywords = keyword.split()

        # 1ì°¨ í•„í„°ë§: ì œëª©ì—ì„œ 1ìˆœìœ„ ë° 3ìˆœìœ„ ì¡°ê±´ í™•ì¸
        articles = soup.find_all('a', class_='news_tit')
        primary_article = next((link for link in articles if all(kw in link.get_text() for kw in keywords)), None)
        secondary_article = next((link for link in articles if any(kw in link.get_text() for kw in keywords)), None)

        # 1ìˆœìœ„ ë˜ëŠ” 3ìˆœìœ„ ì¡°ê±´ ì¶©ì¡± ì‹œ í•´ë‹¹ ë§í¬ ì‚¬ìš©
        article = primary_article if primary_article else secondary_article
        if article:
            link = article['href']
            print(f"ì„ íƒëœ ê¸°ì‚¬ ë§í¬: {link}")

            # ë³¸ë¬¸ ë° ì´ë¯¸ì§€ ì¶”ì¶œ
            driver.get(link)
            WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            article_page = driver.page_source
            article_soup = BeautifulSoup(article_page, 'html.parser')

            paragraphs = article_soup.find_all(['p', 'div', 'span'], class_=lambda x: x and ('content' in x or 'article' in x))
            content = "\n".join([para.get_text().strip() for para in paragraphs if para.get_text().strip()])

            # ë³¸ë¬¸ í™•ì¸: 2ìˆœìœ„ ë° 4ìˆœìœ„ ì¡°ê±´ ì¶©ì¡± ì—¬ë¶€ ê²€ì‚¬
            if primary_article or all(kw in content for kw in keywords) or any(kw in content for kw in keywords):
                print("ë³¸ë¬¸ì— í‚¤ì›Œë“œ ì¡°ê±´ ì¶©ì¡±")
            else:
                content = None

            # ì´ë¯¸ì§€ URL ì¶”ì¶œ (ë©”ì¸ ì´ë¯¸ì§€ í›„ë³´ ì„ íƒ)
            image_tags = article_soup.find_all('img')
            max_width = 0
            image_url = None
            for img in image_tags:
                # alt ì†ì„±ì— ê¸°ì‚¬ ê´€ë ¨ ë‚´ìš©ì´ í¬í•¨ëœ ê²½ìš° ìš°ì„  ê³ ë ¤
                alt_text = img.get('alt', '').lower()
                if any(kw in alt_text for kw in keywords):
                    image_url = img.get('data-src') or img.get('src')
                    break
                
                # widthê°€ ê°€ì¥ í° ì´ë¯¸ì§€ ì„ íƒ
                width = img.get('width')
                if width and width.isdigit():
                    width = int(width)
                    if width > max_width:
                        max_width = width
                        image_url = img.get('data-src') or img.get('src')

            # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬: ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if image_url and not image_url.startswith('http'):
                base_url = driver.current_url.split('/')[0:3]  # í”„ë¡œí† ì½œê³¼ ë„ë©”ì¸ ë¶€ë¶„ ì¶”ì¶œ
                base_url = '/'.join(base_url)
                image_url = base_url + image_url if image_url.startswith('/') else base_url + '/' + image_url

        else:
            print("ì œëª©ê³¼ ë³¸ë¬¸ì— í‚¤ì›Œë“œ ì¡°ê±´ì„ ì¶©ì¡±í•˜ëŠ” ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            content = None
            image_url = None

    finally:
        driver.quit()

    # ê¸°ì‚¬ ìš”ì•½ ë° í˜ë¥´ì†Œë‚˜ í”¼ë“œ ìƒì„±
    if content:
        summarize_and_create_feed(content, image_url, persona_name)
    else:
        print("ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")

# ê¸°ì‚¬ ìš”ì•½ ë° í˜ë¥´ì†Œë‚˜ í”¼ë“œ ìƒì„± í•¨ìˆ˜
def summarize_and_create_feed(content, image_url, persona_name):
    try:
        # KoBART ìš”ì•½ ëª¨ë¸ ë¡œë“œ
        model_name = "gogamza/kobart-summarization"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

        # ê¸°ì‚¬ë¥¼ ìš”ì•½í•˜ì—¬ ê¸¸ì´ë¥¼ ì¤„ì„
        inputs = tokenizer([content], max_length=1024, return_tensors="pt", truncation=True)
        summary_ids = model.generate(inputs["input_ids"], max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)
        summarized_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

        if not summarized_text:
            print("ìš”ì•½ëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        # OpenAI APIë¥¼ ì‚¬ìš©í•´ í˜ë¥´ì†Œë‚˜ ì„±ê²© ë°˜ì˜í•œ í”¼ë“œ ì‘ì„±
        openai.api_key = os.getenv("OPENAI_API_KEY")
        if not openai.api_key:
            print("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        persona_descriptions = {
            "joy": "í•­ìƒ ê¸ì •ì ì´ê³  ë‚™ê´€ì ì¸ ì‹œê°ìœ¼ë¡œ ì„¸ìƒì„ ë°”ë¼ë³´ë©°, ëª¨ë“  ì¼ì—ì„œ ì¢‹ì€ ì ì„ ì°¾ìœ¼ë ¤ ë…¸ë ¥í•©ë‹ˆë‹¤.",
            "anger": "ë¶ˆì˜ì— ëŒ€í•´ ì°¸ì§€ ëª»í•˜ê³  ì‰½ê²Œ í™”ë¥¼ ë‚´ë©°, ì§ì„¤ì ì´ê³  ê³µê²©ì ì¸ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ê³¤ í•©ë‹ˆë‹¤.",
            "disgust": "ëª¨ë“  ê²ƒì— ëŒ€í•´ ë¹„íŒì ì´ê³  ëƒ‰ì†Œì ì¸ íƒœë„ë¥¼ ë³´ì´ë©°, ë‚ ì¹´ë¡œìš´ ì§€ì ì„ ì¦ê¹ë‹ˆë‹¤.",
            "sadness": "ì„¸ìƒì˜ ì–´ë‘ìš´ ë©´ì„ ìì£¼ ë³´ë©°, ì‘ì€ ì¼ì—ë„ ì‰½ê²Œ ìš°ìš¸í•´ì§€ê³  ëˆˆë¬¼ì„ í˜ë¦¬ê³¤ í•©ë‹ˆë‹¤.",
            "fear": "í•™ì‹ì´ ë†’ê³  í’ˆí–‰ì´ ë°”ë¥´ë©°, ë„ë•ì  ê¸°ì¤€ì´ ë†’ì•„ ëª¨ë“  ì¼ì„ ìœ¤ë¦¬ì  ê´€ì ì—ì„œ íŒë‹¨í•©ë‹ˆë‹¤."
        }

        emoji_map = {
            "joy": "ğŸ˜ŠğŸ‘",
            "anger": "ğŸ˜¡ğŸ‘Š",
            "disgust": "ğŸ˜’ğŸ™„",
            "sadness": "ğŸ˜¢ğŸ’”",
            "fear": "ğŸ¤®ğŸ“š"
        }

        temperature_map = {
            "joy": 0.9,
            "anger": 0.8,
            "disgust": 0.7,
            "sadness": 0.6,
            "fear": 0.5
        }

        if persona_name not in persona_descriptions:
            print(f"ì•Œ ìˆ˜ ì—†ëŠ” í˜ë¥´ì†Œë‚˜: {persona_name}")
            return

        selected_emoji = emoji_map.get(persona_name, "")
        selected_temperature = temperature_map.get(persona_name, 0.7)

        persona_prompt = (
            f"ê¸°ì‚¬ ìš”ì•½: \n{summarized_text}\n\n"
            f"ë‹¹ì‹ ì€ '{persona_name}'ë¼ëŠ” ìºë¦­í„°ì…ë‹ˆë‹¤. {persona_descriptions[persona_name]} "
            f"ì´ëŸ° ì„±ê²©ì˜ {persona_name}ê°€ ìœ„ ê¸°ì‚¬ë¥¼ ì½ê³  ê¸€ì„ ì‘ì„±í•œë‹¤ê³  ìƒê°í•˜ê³ , "
            f"ì´ëª¨í‹°ì½˜({selected_emoji})ì„ í¬í•¨í•´ 300ì ë‚´ì™¸ë¡œ ì˜ê²¬ì´ ì•ˆ ëŠê¸°ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”. "
            f"ë¶ˆí•„ìš”í•œ ë§ì€ í•˜ì§€ ë§ê³  í•µì‹¬ë§Œ ë§í•´ì£¼ì„¸ìš”."
        )

        response = openai.ChatCompletion.create(
            model="gpt-4o-mini", #gpt-3.5-turbo gpt-4o-mini
            messages=[
                {"role": "user", "content": persona_prompt}
            ],
            temperature=selected_temperature
        )

        if response and response.choices:
            persona_feed = response['choices'][0]['message']['content'].strip()
            print(f"\n{persona_name}ì˜ í”¼ë“œ:\n{persona_feed}")
        else:
            print(f"\n{persona_name}ì˜ í”¼ë“œ: OpenAI API ì‘ë‹µì—ì„œ í”¼ë“œ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        if image_url:
            print(f"ê¸°ì‚¬ ì´ë¯¸ì§€: {image_url}")

    except Exception as e:
        print(f"í”¼ë“œ ì‘ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ë° ì‹œì‘
scheduler = BackgroundScheduler()
scheduler.add_job(fetch_trending_keywords, 'cron', hour='0,12')  # ë§¤ì¼ 0ì‹œ, 12ì‹œì— ì‹¤í–‰
scheduler.start()

# ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ ì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ ì •ë¦¬
atexit.register(lambda: scheduler.shutdown())

# ì½˜ì†”ì—ì„œ í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    print("ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ê°€ì ¸ì˜¤ê¸° ì‹œì‘!")
    fetch_trending_keywords()
